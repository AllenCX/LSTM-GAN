{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "WORD_SIZE = 5\n",
    "MAX_FILTER_SIZE = 6\n",
    "START_WORD_VEC = np.asarray([1,0], dtype=np.float32).reshape([1,2])\n",
    "END_WORD_VEC = np.asarray([0,1], dtype=np.float32).reshape([1,2])\n",
    "END_WORD_ID = 1\n",
    "START_WORD_ID = 0\n",
    "\n",
    "EMBEDDING_MATRIX, ID2WORD, WORD2ID = pickle.load(open(\"embedding_matrix_2D\",'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id2onehot(word_id):\n",
    "    vec = np.zeros([WORD_SIZE], dtype=np.float32)\n",
    "    vec[word_id] = 1.0\n",
    "    return vec\n",
    "def onehot2id(vec):\n",
    "    index = vec.argsort()[-1]\n",
    "    return index\n",
    "\n",
    "def embedding_look_up(word_id):\n",
    "    word = WORD_DIC.get(word_id)\n",
    "    return word_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocabulary_size, num_nodes, \n",
    "                 batch_size, name=\"\"):\n",
    "        '''\n",
    "        the LSTM process\n",
    "        '''\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reuse = False\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "    \n",
    "    def model(self, input_sentences, vocabulary_size, num_nodes,\n",
    "             batch_size):\n",
    "        #to make sure nothing goes wrong...\n",
    "        assert input_sentences[0].shape[0] == batch_size\n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_var\".format(self.name) ,reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.constant_initializer(0), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.constant_initializer(0), \n",
    "                                                       trainable=False)\n",
    "  \n",
    "        self.train_data = input_sentences\n",
    "    \n",
    "        #Since now input_sentence is a list of word vectors, \n",
    "        #we do not need following 3 lines anymore.\n",
    "        \n",
    "        self.train_inputs = self.train_data\n",
    "        \n",
    "        self.outputs = list()\n",
    "        self.output = self.saved_output\n",
    "        self.state = self.saved_state\n",
    "        \n",
    "        self.state_list = list()\n",
    "        self.state_list.append(self.saved_state)\n",
    "        \n",
    "        for i in self.train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(self.output)\n",
    "            self.state_list.append(self.state)\n",
    "            \n",
    "        self.saved_state = self.saved_state.assign(self.state) \n",
    "        self.saved_output = self.saved_output.assign(self.output)\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                           scope=\"{}_LSTM_var\".format(self.name))\n",
    "        \n",
    "        self.reuse = True\n",
    "        \n",
    "        return self.outputs\n",
    "    def lstm_cell(self, inputs, hidden_layer, state): \n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state   \n",
    "    \n",
    "    '''def lstm_process(self, inputs, hidden_layer, state):\n",
    "        for i in train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(output)\n",
    "\n",
    "        return outputs'''\n",
    "    \n",
    "    '''with tf.control_dependencies([self.saved_output.assign(output),\n",
    "                                     self.saved_state.assign(state)]):\n",
    "            \n",
    "            logits = tf.matmul(tf.concat(0, outputs), self.w) + self.b\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, logits), \n",
    "                                                        tf.concat(0, train_labels), name=\"loss\"))\n",
    "            \n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            gradient, v = zip(*optimizer.compute_gradients(loss))\n",
    "            \n",
    "            \n",
    "            tf.add_to_collection(\"loss\", loss)\n",
    "            tf.add_to_collection(\"logits\", logits)\n",
    "            tf.add_to_collection(\"train_labels\", train_labels)\n",
    "            \n",
    "        '''\n",
    "        #It seems we do not need those lines above\n",
    "        \n",
    "    def __call__(self, input_sentences):\n",
    "        return self.model(input_sentences, self.vocabulary_size, self.num_nodes,\n",
    "                  self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gen_LSTM():\n",
    "    def __init__(self, bi_hidden, batch_size, vocabulary_size, num_nodes, max_len=32, name=\"\"):\n",
    "        self.bi_hidden = bi_hidden\n",
    "        self.name = name\n",
    "        self.reuse = False\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "    def model(self, bi_hidden, batch_size, vocabulary_size, num_nodes, max_len):\n",
    "        \n",
    "        #assert bi_hidden[0].get_shape()[0] == batch_size\n",
    "        #assert bi_hidden[0].get_shape()[1] == bi_hidden\n",
    "        \n",
    "        with tf.variable_scope(\"{}_gen_LSTM\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "                self.ic = tf.get_variable(\"ic\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.fc = tf.get_variable(\"fc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.oc = tf.get_variable(\"oc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.cc = tf.get_variable(\"cc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                \n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                self.w = tf.get_variable(\"w\", [num_nodes, WORD_SIZE], tf.float32,\n",
    "                                        tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.b = tf.get_variable(\"b\", [WORD_SIZE], tf.float32,\n",
    "                                        tf.constant_initializer(0))\n",
    "                \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.constant_initializer(0), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.constant_initializer(0), \n",
    "                                                       trainable=False)\n",
    "        \n",
    "            self.c = self.attention(self.bi_hidden, self.saved_state)\n",
    "        \n",
    "        \n",
    "        self.init_state = self.saved_state\n",
    "        self.state = self.saved_state\n",
    "        self.output = self.saved_output\n",
    "        self.states = list()\n",
    "        self.states.append(self.saved_state)\n",
    "        self.outputs = list()\n",
    "        self.predicted_sentence = list()\n",
    "        self.logits = list()\n",
    "        \n",
    "        #generate words\n",
    "        for i in range(self.max_len):\n",
    "            #There must be bugs in the following lines, needed to be fixed later\n",
    "            if(i == 0):\n",
    "                #input_word = id2onehot(START_WORD_ID).reshape([batch_size, WORD_SIZE])\n",
    "                input_word = np.asarray([1.,0.]).reshape([batch_size, self.vocabulary_size])\n",
    "                input_word = tf.cast(input_word, dtype=tf.float32)\n",
    "                print(\"input word at step {}:{}\".format(i, input_word))\n",
    "            else:\n",
    "                input_word = tf.nn.embedding_lookup(EMBEDDING_MATRIX, self.predicted_sentence[-1])\n",
    "                input_word = tf.reshape(input_word, [batch_size, self.vocabulary_size])\n",
    "                #without tf.cast there will be an exception\n",
    "                input_word = tf.cast(input_word, dtype=tf.float32, name=\"input_word\")\n",
    "                print(\"at step {} predicted word embedding:{}\".format(i, input_word))\n",
    "                \n",
    "            self.output, self.state = self.gen_lstm_cell(input_word, self.output, self.state, self.c)\n",
    "            self.outputs.append(self.output)\n",
    "            self.states.append(self.state)\n",
    "            logits = tf.matmul(self.output, self.w) + self.b\n",
    "            self.logits.append(logits)\n",
    "            prediction = tf.argmax(tf.nn.softmax(logits), 1, name=\"predicted_word\")\n",
    "            self.predicted_sentence.append(prediction[0])\n",
    "            print(\"predicted word:{}\".format(prediction))\n",
    "            if tf.cast(tf.equal(prediction, END_WORD_ID), tf.int64) == 1:\n",
    "                print(\"detected the END_WORD and break loop\")\n",
    "                break\n",
    "            if i == max_len - 1:\n",
    "                print(\"check point for max_len - 1, max_len={}\".format(self.max_len))\n",
    "                self.predicted_sentence.append(tf.constant(END_WORD_ID, dtype=tf.int64))\n",
    "            \n",
    "        \n",
    "        self.saved_state = self.saved_state.assign(self.state)\n",
    "        self.saved_output = self.saved_output.assign(self.output)\n",
    "        \n",
    "        \n",
    "        #self.logits = tf.matmul(tf.concat(0, self.outputs), self.w) + self.b\n",
    "        self.reuse = True\n",
    "        self.lstm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                               scope=\"{}_gen_LSTM\".format(self.name))\n",
    "        self.variables = self.lstm_variables\n",
    "        \n",
    "        return self.predicted_sentence\n",
    "    \n",
    "    def gen_lstm_cell(self, inputs, hidden_layer, state, c): \n",
    "        #single step LSTM with context vector\n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im)\\\n",
    "                                    + tf.matmul(c, self.ic) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm)\\\n",
    "                                    + tf.matmul(c, self.fc) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm)\\\n",
    "                                  + tf.matmul(c, self.cc) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om)\\\n",
    "                                     + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "            \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state\n",
    "        \n",
    "    def attention(self, h, s):\n",
    "        with tf.variable_scope(\"{}_attention_module\".format(self.name), \n",
    "                               reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0))\n",
    "            attn_logits = list()\n",
    "            for h_j in h:\n",
    "                attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            a = tf.reshape(tf.nn.softmax(attn_logits),\n",
    "                           shape=[len(h)], name=\"attention_value\")\n",
    "            c = 0\n",
    "            for j in range(int(a.get_shape()[0])):\n",
    "                c += a[j] * h[j]\n",
    "                \n",
    "            return c\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.model(self.bi_hidden, \n",
    "                          self.batch_size, \n",
    "                          self.vocabulary_size, \n",
    "                          self.num_nodes, \n",
    "                          self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bi_h = list()\\nfor i in range(10):\\n    bi_h.append(tf.Variable(tf.truncated_normal(stddev=0.5, shape=(1,10))))\\n\\ngen_L = gen_LSTM(bi_h, vocabulary_size=5, batch_size=1, num_nodes=5, max_len=32, name=\"test_gen_lstm0\")\\n\\na, b, c, d = gen_L()\\n\\nsess = tf.Session()\\nsess.run(tf.initialize_all_variables())\\nfeed_dict = dict()\\n\\nx,y,z,w = sess.run([a,b,c, d], feed_dict=feed_dict)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bi_h = list()\n",
    "for i in range(10):\n",
    "    bi_h.append(tf.Variable(tf.truncated_normal(stddev=0.5, shape=(1,10))))\n",
    "\n",
    "gen_L = gen_LSTM(bi_h, vocabulary_size=5, batch_size=1, num_nodes=5, max_len=32, name=\"test_gen_lstm0\")\n",
    "\n",
    "a, b, c, d = gen_L()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = dict()\n",
    "\n",
    "x,y,z,w = sess.run([a,b,c, d], feed_dict=feed_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, s, conv_sizes, num_filters=64, dropout_keep_prob=0.5, \n",
    "                 name=\"\"):\n",
    "        '''\n",
    "        The Discriminator, uses Convolutional Neural Network to \n",
    "        classify the input sentence. See \n",
    "        http://www.people.fas.harvard.edu/~yoonkim/data/emnlp_2014.pdf\n",
    "        for detail. Those code below with some inspiration from\n",
    "        [https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py]\n",
    "        \n",
    "        Arguments:\n",
    "        s: the sentence representation caculated by LSTM process, \n",
    "        which should a size of [batch_size, vocabulary_size, length, channel = 1]\n",
    "        '''\n",
    "        self.s = s\n",
    "        self.sequence_length = int(s.get_shape()[1])\n",
    "        self.reuse = False\n",
    "        self.name = name\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.vocabulary_size = int(s.get_shape()[2])\n",
    "        #print(self.hidden_size)\n",
    "        self.pooled_outputs = list()\n",
    "    \n",
    "    def model(self):\n",
    "        with tf.variable_scope(\"{}\".format(self.name), \n",
    "                               reuse=self.reuse):\n",
    "            for i, conv_size in enumerate(self.conv_sizes):\n",
    "                with tf.variable_scope(\"conv_maxpooling_{}\"\n",
    "                                      .format(conv_size), reuse=self.reuse):\n",
    "                    #filter_shape = [conv_size, self.hidden_size, 1, num_filters]\n",
    "                    W = tf.get_variable(\"W\", [conv_size, self.vocabulary_size, 1, self.num_filters],\n",
    "                                       tf.float32,\n",
    "                                       tf.truncated_normal_initializer(stddev=0.1))\n",
    "                    print(\"the size of {} => {}\"\n",
    "                          .format(W.name, W.get_shape()))\n",
    "                    b = tf.get_variable(\"b\", [self.num_filters], tf.float32,\n",
    "                                       tf.constant_initializer(0.1))\n",
    "                    self.s = tf.cast(self.s, dtype=tf.float32)\n",
    "                    conv = tf.nn.conv2d(self.s, W, [1,1,1,1], 'VALID', name=\"conv\")\n",
    "                    print(\"the size of {} => {}\"\n",
    "                          .format(conv.name, conv.get_shape()))\n",
    "                    h = tf.nn.relu(conv + b)\n",
    "                    \n",
    "                    #h = tf.cast(h, dtype=tf.float32, name=\"cas_h_to_32\")\n",
    "                    \n",
    "                    pooled = tf.nn.max_pool(h,\n",
    "                                            ksize=[1, self.sequence_length-conv_size+1,1,1],\n",
    "                                           strides=[1,1,1,1],\n",
    "                                           padding='VALID',\n",
    "                                           name=\"max_pooling\")\n",
    "                \n",
    "                    print(\"the size of pooling layer {}\"\n",
    "                          .format(pooled.get_shape()))\n",
    "                    \n",
    "                self.pooled_outputs.append(pooled)\n",
    "            \n",
    "        self.total_filter_num = self.num_filters * len(self.conv_sizes)\n",
    "        self.h_pool = tf.concat(3, self.pooled_outputs,\n",
    "                                name=\"{}_concat_pooling\".format(self.name))\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.total_filter_num])\n",
    "        print(\"h_pool size => {}\".format(self.h_pool_flat.get_shape()))\n",
    "     \n",
    "        #after convolutional layer, we can define the fully connectedmlayer\n",
    "        \n",
    "        with tf.variable_scope(\"D_fully_connected_layer\", reuse=self.reuse):\n",
    "            w_shape = [self.h_pool_flat.get_shape()[1], 1]\n",
    "            W = tf.get_variable(\"W\", w_shape, tf.float32,\n",
    "                               tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", [1], tf.float32,\n",
    "                               tf.constant_initializer(0.1))\n",
    "            logits = tf.nn.relu(tf.matmul(self.h_pool_flat, W) + b)\n",
    "            print(\"logits size => {}\".format(logits.get_shape()))\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                          scope=\"{}\".format(self.name))\n",
    "        self.variables = self.variables + \\\n",
    "            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                              scope=\"D_fully_connected_layer\")\n",
    "        self.reuse = True\n",
    "        \n",
    "        return logits\n",
    "    def __call__(self):\n",
    "        return self.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMGAN:\n",
    "    def __init__(self, input_sentence, max_len=32, vocabulary_size=200, \n",
    "                 num_nodes=128, batch_size=16, name=\"defaultname\"):\n",
    "        self.input_sentence = input_sentence\n",
    "        self.batch_size = batch_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.name = name\n",
    "        self.max_len = max_len\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.reversed_sentence = self.reverse_sentence(self.input_sentence)\n",
    "        self.reuse = False\n",
    "        \n",
    "        #Generating sentences\n",
    "        self.L_forward = LSTM(self.vocabulary_size, self.num_nodes, self.batch_size, \n",
    "                         name=\"forward_{}\".format(self.name))\n",
    "        self.L_backward = LSTM(self.vocabulary_size, self.num_nodes, self.batch_size,\n",
    "                          name=\"backward_{}\".format(self.name))\n",
    "        \n",
    "        self.forward_LSTM_outputs = self.L_forward(self.input_sentence)\n",
    "        self.backward_LSTM_outputs = self.L_backward(self.reversed_sentence)\n",
    "        #concat the two outputs \n",
    "        self.backward_LSTM_outputs = self.reverse_sentence(self.backward_LSTM_outputs)\n",
    "        self.BiLSTM_outputs = list()\n",
    "        for i in range(len(self.forward_LSTM_outputs)):\n",
    "            self.BiLSTM_outputs.append(\n",
    "                tf.concat(\n",
    "                    1,[self.forward_LSTM_outputs[i], self.backward_LSTM_outputs[i]]))\n",
    "        \n",
    "        self.gen_LSTM = gen_LSTM(self.BiLSTM_outputs, self.batch_size, \n",
    "                                 self.vocabulary_size, self.num_nodes, \n",
    "                                 max_len=self.max_len, name=\"GEN_{}\".format(self.name))\n",
    "        self.g_sentence = self.gen_LSTM()\n",
    "        self.g_sentence_len = len(self.g_sentence)\n",
    "        self.g_embedding_sentence = tf.nn.embedding_lookup(EMBEDDING_MATRIX, \n",
    "                                                          self.g_sentence)\n",
    "        self.L_forward_vars = self.L_forward.variables\n",
    "        self.L_backward_vars = self.L_backward.variables\n",
    "        self.g_vars = self.gen_LSTM.variables\n",
    "        \n",
    "        self.d_input_sentence = tf.cast(self.g_embedding_sentence, dtype=tf.float32)\n",
    "        self.d_input_sentence = tf.reshape(\n",
    "            self.g_embedding_sentence, \n",
    "            [self.batch_size, self.g_sentence_len, self.vocabulary_size, 1], \n",
    "            name=\"{}_d_input_sentence\".format(self.name))\n",
    "        \n",
    "        #Discriminating sentence\n",
    "        self.D = Discriminator(self.d_input_sentence,\n",
    "                               conv_sizes=[3,4],\n",
    "                               num_filters=64,\n",
    "                               dropout_keep_prob=0.5,\n",
    "                               name=\"{}_D\".format(self.name))\n",
    "        self.outputs_from_g = self.D()\n",
    "        self.d_vars = self.D.variables\n",
    "        \n",
    "        '''for i in range(self.g_sentence_length):\n",
    "            self.g_sentence_embedding.append(\n",
    "                tf.nn.embedding_lookup(\n",
    "                    EMBEDDING_MATRIX, self.g_sentence[i]))'''\n",
    "        \n",
    "        self.reuse = True\n",
    "    def reverse_sentence(self, inputs):\n",
    "        length = len(inputs)\n",
    "        ret = list()\n",
    "        #reverse\n",
    "        for i in range(length):\n",
    "            ret.append(inputs[length-i-1])\n",
    "        return ret\n",
    "    def pad_sentence(self, input_sentence):\n",
    "        #add zero at the end of the sentence\n",
    "        if len(input_sentence) >= MAX_FILTER_SIZE:\n",
    "            return input_sentence\n",
    "        else:\n",
    "            for i in range(MAX_FILTER_SIZE - len(input_sentence)):\n",
    "                input_sentence.append(np.zeros(WORD_SIZE))\n",
    "            return input_sentence\n",
    "    \n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-19 17:44:30,098 : INFO : loading Word2Vec object from models/word2vec_2D.model\n",
      "2016-10-19 17:44:30,336 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-19 17:44:30,340 : INFO : setting ignored attribute cum_table to None\n"
     ]
    }
   ],
   "source": [
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec_2D.model\")\n",
    "\n",
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n",
    "\n",
    "for i, _ in enumerate(test_sentence):\n",
    "    test_sentence[i] = test_sentence[i].reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word at step 0:Tensor(\"Cast:0\", shape=(1, 2), dtype=float32)\n",
      "predicted word:Tensor(\"predicted_word:0\", shape=(1,), dtype=int64)\n",
      "at step 1 predicted word embedding:Tensor(\"input_word:0\", shape=(1, 2), dtype=float32)\n",
      "predicted word:Tensor(\"predicted_word_1:0\", shape=(1,), dtype=int64)\n",
      "at step 2 predicted word embedding:Tensor(\"input_word_1:0\", shape=(1, 2), dtype=float32)\n",
      "predicted word:Tensor(\"predicted_word_2:0\", shape=(1,), dtype=int64)\n",
      "at step 3 predicted word embedding:Tensor(\"input_word_2:0\", shape=(1, 2), dtype=float32)\n",
      "predicted word:Tensor(\"predicted_word_3:0\", shape=(1,), dtype=int64)\n",
      "check point for max_len - 1, max_len=4\n",
      "the size of LSTMGAN_D/conv_maxpooling_3/W:0 => (3, 2, 1, 64)\n",
      "the size of LSTMGAN_D/conv_maxpooling_3/conv:0 => (1, 3, 1, 64)\n",
      "the size of pooling layer (1, 1, 1, 64)\n",
      "the size of LSTMGAN_D/conv_maxpooling_4/W:0 => (4, 2, 1, 64)\n",
      "the size of LSTMGAN_D/conv_maxpooling_4/conv:0 => (1, 2, 1, 64)\n",
      "the size of pooling layer (1, 1, 1, 64)\n",
      "h_pool size => (1, 128)\n",
      "logits size => (1, 1)\n"
     ]
    }
   ],
   "source": [
    "G = LSTMGAN(test_sentence, max_len=4, vocabulary_size=2, num_nodes=5, batch_size=1, name=\"LSTMGAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMGAN_D/conv_maxpooling_3/W:0\n",
      "LSTMGAN_D/conv_maxpooling_3/b:0\n",
      "LSTMGAN_D/conv_maxpooling_4/W:0\n",
      "LSTMGAN_D/conv_maxpooling_4/b:0\n",
      "D_fully_connected_layer/W:0\n",
      "D_fully_connected_layer/b:0\n"
     ]
    }
   ],
   "source": [
    "for i in G.d_vars:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    sss = sess.run(G.d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = Discriminator(s_d, [3], 64, 0.5, False,\"ddd1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST\")\n",
    "A = attention(h_j, s_j, \"143\")\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    feed_dict = dict()\n",
    "    #for i in range(3):\n",
    "        #feed_dict[L.train_data[i]] = np.reshape(test_sentence[i],(1,200))\n",
    "    outc, outa = sess.run([A.c, A.a], feed_dict=feed_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
