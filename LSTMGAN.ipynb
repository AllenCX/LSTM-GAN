{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "WORD_SIZE = 5\n",
    "MAX_FILTER_SIZE = 6\n",
    "START_WORD_VEC = np.asarray([1,0,0,0,0], dtype=np.float32).reshape([1,5])\n",
    "END_WORD_VEC = np.asarray([0,0,0,0,1], dtype=np.float32).reshape([1,5])\n",
    "END_WORD_ID = 4\n",
    "START_WORD_ID = 0\n",
    "\n",
    "embedding_matrix, id2word, word2id = pickle.load(open(\"embedding_matrix_2D\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_embedding_matrix = tf.constant(np.asarray([[1,0,0,0,0],\n",
    "                                   [0,1,0,0,0],\n",
    "                                   [0,0,1,0,0],\n",
    "                                  [0,0,0,1,0],\n",
    "                                  [0,0,0,0,1]], dtype=np.float32), name=\"tmp_embedding_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id2onehot(word_id):\n",
    "    vec = np.zeros([WORD_SIZE], dtype=np.float32)\n",
    "    vec[word_id] = 1.0\n",
    "    return vec\n",
    "def onehot2id(vec):\n",
    "    index = vec.argsort()[-1]\n",
    "    return index\n",
    "\n",
    "def embedding_look_up(word_id):\n",
    "    word = WORD_DIC.get(word_id)\n",
    "    return word_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocabulary_size=200, num_nodes=128, \n",
    "                 batch_size=20, num_unrollings=2, name=\"\"):\n",
    "        '''\n",
    "        the LSTM process\n",
    "        '''\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reuse = False\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "    \n",
    "    def model(self, input_sentences, vocabulary_size, num_nodes,\n",
    "             batch_size):\n",
    "        with tf.variable_scope(\"{}_LSTM_var\".format(self.name) ,reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.constant_initializer(0), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.constant_initializer(0), \n",
    "                                                       trainable=False)\n",
    "            '''self.saved_state_backward = tf.get_variable(\"saved_state_backward\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False, name=\"backward_state\")'''\n",
    "  \n",
    "        self.train_data = input_sentences\n",
    "        #Since now input_sentence is a list of word vectors, \n",
    "        #we do not need following 3 lines anymore.\n",
    "        '''for _ in range(self.num_unrollings + 1):\n",
    "            self.train_data.append(\n",
    "                tf.placeholder(tf.float32, shape=[self.batch_size, self.vocabulary_size]))'''\n",
    "        self.train_inputs = self.train_data[:-1]\n",
    "        self.train_labels = self.train_data[1:]\n",
    "        \n",
    "        self.outputs = list()\n",
    "        self.output = self.saved_output\n",
    "        self.state = self.saved_state\n",
    "        \n",
    "        self.state_list = list()\n",
    "        self.state_list.append(self.saved_state)\n",
    "        \n",
    "        for i in self.train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(self.output)\n",
    "            self.state_list.append(self.state)\n",
    "            \n",
    "        self.saved_state = self.saved_state.assign(self.state) \n",
    "        self.saved_output = self.saved_output.assign(self.output)\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        \n",
    "        self.reuse = True\n",
    "        \n",
    "        return self.outputs\n",
    "    def lstm_cell(self, inputs, hidden_layer, state): \n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state   \n",
    "    \n",
    "    '''def lstm_process(self, inputs, hidden_layer, state):\n",
    "        for i in train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(output)\n",
    "\n",
    "        return outputs'''\n",
    "    \n",
    "    '''with tf.control_dependencies([self.saved_output.assign(output),\n",
    "                                     self.saved_state.assign(state)]):\n",
    "            \n",
    "            logits = tf.matmul(tf.concat(0, outputs), self.w) + self.b\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, logits), \n",
    "                                                        tf.concat(0, train_labels), name=\"loss\"))\n",
    "            \n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            gradient, v = zip(*optimizer.compute_gradients(loss))\n",
    "            \n",
    "            \n",
    "            tf.add_to_collection(\"loss\", loss)\n",
    "            tf.add_to_collection(\"logits\", logits)\n",
    "            tf.add_to_collection(\"train_labels\", train_labels)\n",
    "            \n",
    "        '''\n",
    "        #It seems we do not need those lines above\n",
    "        \n",
    "    def __call__(self, input_sentences):\n",
    "        return self.model(input_sentences, self.vocabulary_size, self.num_nodes,\n",
    "                  self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-11 22:09:49,628 : INFO : loading Word2Vec object from models/word2vec_2D.model\n",
      "2016-10-11 22:09:49,857 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-11 22:09:49,859 : INFO : setting ignored attribute cum_table to None\n"
     ]
    }
   ],
   "source": [
    "L = LSTM(vocabulary_size=2, batch_size=1, num_unrollings=2, num_nodes=4, name=\"TEST4\")\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec_2D.model\")\n",
    "\n",
    "test_word1 = np.reshape(word_model[\"china\"], (1,2))\n",
    "test_word2 = np.reshape(word_model[\"uk\"], (1, 2))\n",
    "test_word3 = np.reshape(word_model[\"america\"], (1, 2))\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sess = tf.Session()\\nsess.run(tf.initialize_all_variables())\\nsess.run(L(test_sentence))\\n\\nsess.run(L.state)\\n\\nsess.run(L.saved_state)\\n\\nsess.run(L.state_list)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(L(test_sentence))\n",
    "\n",
    "sess.run(L.state)\n",
    "\n",
    "sess.run(L.saved_state)\n",
    "\n",
    "sess.run(L.state_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gen_LSTM():\n",
    "    def __init__(self, bi_hidden, batch_size, vocabulary_size, num_nodes, max_len=32, name=\"\"):\n",
    "        self.bi_hidden = bi_hidden\n",
    "        self.name = name\n",
    "        self.reuse = False\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "    def model(self, bi_hidden, batch_size, vocabulary_size, num_nodes, max_len):\n",
    "        with tf.variable_scope(\"{}_gen_LSTM\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "                self.ic = tf.get_variable(\"ic\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.fc = tf.get_variable(\"fc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.oc = tf.get_variable(\"oc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.cc = tf.get_variable(\"cc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                \n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                self.w = tf.get_variable(\"w\", [num_nodes, WORD_SIZE], tf.float32,\n",
    "                                        tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.b = tf.get_variable(\"b\", [WORD_SIZE], tf.float32,\n",
    "                                        tf.constant_initializer(0))\n",
    "                \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.constant_initializer(0), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.constant_initializer(0), \n",
    "                                                       trainable=False)\n",
    "        \n",
    "            self.c = self.attention(self.bi_hidden, self.saved_state)\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        \n",
    "        self.init_state = self.saved_state\n",
    "        self.state = self.saved_state\n",
    "        self.output = self.saved_output\n",
    "        self.states = list()\n",
    "        self.states.append(self.saved_state)\n",
    "        self.outputs = list()\n",
    "        self.predicted_sentence = list()\n",
    "        self.logits = list()\n",
    "        self.reuse = True\n",
    "        \n",
    "        \n",
    "        #print(self.predicted_sentence)\n",
    "        for i in range(10):\n",
    "            print(\"i=>{}\".format(i))\n",
    "            \n",
    "            if(i == 0):\n",
    "                input_word = id2onehot(START_WORD_ID).reshape([batch_size, WORD_SIZE])\n",
    "            else:\n",
    "                #print(\"at step predicted word:{}\".format(self.predicted_sentence[-1]))\n",
    "                input_word = tf.nn.embedding_lookup(tmp_embedding_matrix, self.predicted_sentence[-1])\n",
    "                input_word = tf.reshape(input_word, [batch_size, WORD_SIZE])\n",
    "                print(\"at step predicted word:{}\".format(input_word))\n",
    "            self.output, self.state = self.gen_lstm_cell(input_word, self.output, self.state, self.c)\n",
    "            self.outputs.append(self.output)\n",
    "            #return self.output,self.state\n",
    "            self.states.append(self.state)\n",
    "            logits = tf.matmul(self.output, self.w) + self.b\n",
    "            self.logits.append(logits)\n",
    "            prediction = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "            self.predicted_sentence.append(prediction[0])\n",
    "            #return tf.equal(prediction, END_WORD_ID)\n",
    "            print(\"check point for tf.equal\")\n",
    "            #return tf.cast(tf.equal(prediction, END_WORD_ID)[0], tf.int32)\n",
    "            if tf.cast(tf.equal(prediction, END_WORD_ID), tf.int32) == 1:\n",
    "                print(\"detected the END_WORD and break loop\")\n",
    "                break\n",
    "            if i == max_len - 1:\n",
    "                print(\"check point for max_len - 1\")\n",
    "                self.predicted_sentence.append(END_WORD_ID)\n",
    "            \n",
    "        \n",
    "        self.saved_state = self.saved_state.assign(self.state)\n",
    "        self.saved_output = self.saved_output.assign(self.output)\n",
    "        '''with tf.control_dependencies([self.saved_state.assign(self.state), \n",
    "                                      self.saved_output.assign(self.output)]):'''\n",
    "        \n",
    "        #self.logits = tf.matmul(tf.concat(0, self.outputs), self.w) + self.b\n",
    "        self.reuse = True\n",
    "        print(\"the last check point\")\n",
    "        return self.logits, self.predicted_sentence, self.states, self.saved_state\n",
    "    \n",
    "    def gen_lstm_cell(self, inputs, hidden_layer, state, c): \n",
    "        #single step LSTM with context vector\n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im)\\\n",
    "                                    + tf.matmul(c, self.ic) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm)\\\n",
    "                                    + tf.matmul(c, self.fc) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm)\\\n",
    "                                  + tf.matmul(c, self.cc) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om)\\\n",
    "                                     + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "            \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state\n",
    "        \n",
    "    def attention(self, h, s):\n",
    "        with tf.variable_scope(\"attention_module\", reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            attn_logits = list()\n",
    "            for h_j in h:\n",
    "                attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            a = tf.reshape(tf.nn.softmax(attn_logits),\n",
    "                           shape=[len(h)], name=\"attention_value\")\n",
    "            c = 0\n",
    "            for j in range(int(a.get_shape()[0])):\n",
    "                c += a[j] * h[j]\n",
    "                \n",
    "            return c\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.model(self.bi_hidden, self.batch_size, self.vocabulary_size, self.num_nodes, self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bi_h = list()\n",
    "for i in range(10):\n",
    "    bi_h.append(tf.Variable(tf.truncated_normal(stddev=0.5, shape=(1,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_L = gen_LSTM(bi_h, vocabulary_size=5, batch_size=1, num_nodes=5, max_len=32, name=\"test_gen_lstm0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=>0\n",
      "check point for tf.equal\n",
      "i=>1\n",
      "at step predicted word:Tensor(\"Reshape:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>2\n",
      "at step predicted word:Tensor(\"Reshape_1:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>3\n",
      "at step predicted word:Tensor(\"Reshape_2:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>4\n",
      "at step predicted word:Tensor(\"Reshape_3:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>5\n",
      "at step predicted word:Tensor(\"Reshape_4:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>6\n",
      "at step predicted word:Tensor(\"Reshape_5:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>7\n",
      "at step predicted word:Tensor(\"Reshape_6:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>8\n",
      "at step predicted word:Tensor(\"Reshape_7:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "i=>9\n",
      "at step predicted word:Tensor(\"Reshape_8:0\", shape=(1, 5), dtype=float32)\n",
      "check point for tf.equal\n",
      "the last check point\n"
     ]
    }
   ],
   "source": [
    "a, b, c, d = gen_L()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = sess.run([c, d], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21270046,  0.06521814,  0.20149739,  0.05270585,  0.28610146]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(gen_L.init_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.0630172 , -0.00347754,  0.07997488, -0.03464898,  0.07956603]], dtype=float32),\n",
       " array([[ 0.13928902,  0.03132836,  0.1446476 ,  0.01087406,  0.17840704]], dtype=float32),\n",
       " array([[ 0.1767053 ,  0.04861825,  0.17514524,  0.0325804 ,  0.23023163]], dtype=float32),\n",
       " array([[ 0.19509576,  0.05713952,  0.18939164,  0.04299659,  0.25726599]], dtype=float32),\n",
       " array([[ 0.20415184,  0.06132318,  0.19600181,  0.0480244 ,  0.27135199]], dtype=float32),\n",
       " array([[ 0.20861542,  0.06337228,  0.1990501 ,  0.05046887,  0.27869168]], dtype=float32),\n",
       " array([[ 0.21081622,  0.06437417,  0.20044692,  0.05166727,  0.28251815]], dtype=float32),\n",
       " array([[ 0.21190146,  0.06486335,  0.20108247,  0.05226019,  0.28451449]], dtype=float32),\n",
       " array([[ 0.21243659,  0.06510192,  0.20136926,  0.0525564 ,  0.28555682]], dtype=float32),\n",
       " array([[ 0.21270046,  0.06521814,  0.20149739,  0.05270585,  0.28610146]], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class attention:\n",
    "    def __init__(self, h, s, name=\"\"):\n",
    "        '''\n",
    "        Caculate the attention weight.\n",
    "        TODO:\n",
    "            I think it seems appropriate to transform the class \n",
    "            into a function...\n",
    "        arguments:\n",
    "            h: all the encoder's hidden states, \n",
    "            each of them has a size of [2*Hidden_size, Batch_size]\n",
    "            s: [2*Hidden_size, Batch_size], decoder's hidden state\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.attn_logits = list()\n",
    "        self.a = list()\n",
    "        self.reuse = False\n",
    "        \n",
    "        #some variables\n",
    "        with tf.variable_scope(\"{}_attention\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            #caculate the attention weight\n",
    "            for h_j in h:\n",
    "                self.attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                self.attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, self.attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            self.a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                                shape=[len(h)], name=\"attention_value\")\n",
    "            self.c = 0\n",
    "            for j in range(self.a.get_shape()[0]):\n",
    "                self.c += self.a[j] * h[j]\n",
    "        self.reuse = True\n",
    "    def get_attention():\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, s, conv_sizes, num_filters, dropout_keep_prob,\n",
    "                 reuse=False, name=\"\"):\n",
    "        '''\n",
    "        The Discriminator, uses Convolutional Neural Network to \n",
    "        classify the input sentence. See \n",
    "        http://www.people.fas.harvard.edu/~yoonkim/data/emnlp_2014.pdf\n",
    "        for detail. Those code below with some inspiration from\n",
    "        [https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py]\n",
    "        \n",
    "        Arguments:\n",
    "        s: the sentence representation caculated by LSTM process, \n",
    "        which has a size of [batch_size, hidden_layer_size]\n",
    "        '''\n",
    "        self.sequence_length = int(s.get_shape()[1])\n",
    "        self.reuse = reuse\n",
    "        self.name = name\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.hidden_size = int(s.get_shape()[2])\n",
    "        #print(self.hidden_size)\n",
    "        self.pooled_outputs = list()\n",
    "        for i, conv_size in enumerate(conv_sizes):\n",
    "            with tf.variable_scope(\"D_conv_maxpooling_{}\"\n",
    "                                  .format(conv_size), reuse=self.reuse):\n",
    "                #filter_shape = [conv_size, self.hidden_size, 1, num_filters]\n",
    "                W = tf.get_variable(\"W\", [conv_size, self.hidden_size, 1, num_filters],\n",
    "                                   tf.float32,\n",
    "                                   tf.truncated_normal_initializer(stddev=0.1))\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(W.name, W.get_shape()))\n",
    "                b = tf.get_variable(\"b\", [num_filters], tf.float32,\n",
    "                                   tf.constant_initializer(0.1))\n",
    "                \n",
    "                conv = tf.nn.conv2d(s, W, [1,1,1,1], 'VALID', name=\"conv\")\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(conv.name, conv.get_shape()))\n",
    "                h = tf.nn.relu(conv + b)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                       ksize=[1, self.sequence_length-conv_size+1, 1, 1],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='VALID',\n",
    "                                       name=\"max_pooling\")\n",
    "                print(\"the size of pooling layer {}\"\n",
    "                      .format(pooled.get_shape()))\n",
    "            self.pooled_outputs.append(pooled)\n",
    "            \n",
    "        self.total_filter_num = num_filters * len(conv_sizes)\n",
    "        self.h_pool = tf.concat(3, self.pooled_outputs,\n",
    "                                name=\"{}_concat_pooling_layer\".format(self.name))\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.total_filter_num])\n",
    "        print(\"h_pool size => {}\".format(self.h_pool_flat.get_shape()))\n",
    "     \n",
    "        #after convolutional layer, we can define the fully connectedmlayer\n",
    "        \n",
    "        with tf.variable_scope(\"D_fully_connected_layer\", reuse=self.reuse):\n",
    "            w_shape = [self.h_pool_flat.get_shape()[1], 1]\n",
    "            W = tf.get_variable(\"W\", w_shape, tf.float32,\n",
    "                               tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", [1], tf.float32,\n",
    "                               tf.constant_initializer(0.1))\n",
    "            logits = tf.nn.relu(tf.matmul(self.h_pool_flat, W) + b)\n",
    "            print(\"logits size => {}\".format(logits.get_shape()))\n",
    "            \n",
    "        self.reuse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, input_sentence, max_len=32, vocabulary_size=200, \n",
    "                 num_nodes=128, batch_size=16, name=\"\"):\n",
    "        self.input_sentence = input_sentence\n",
    "        self.reverse_sentence = self.reverse_sentence(self.input_sentence)\n",
    "        self.forward_LSTM_outputs = LSTM(self.input_sentence, \n",
    "                                        name=\"forward\")\n",
    "        self.backward_LSTM_outputs = LSTM(self.reverse_sentence,\n",
    "                                         name=\"backward\")     \n",
    "        #concat the two outputs \n",
    "        \n",
    "        #using attention\n",
    "        self.gen_LSTM = gen_LSTM()\n",
    "        self.attention = attention()\n",
    "        \n",
    "    def reverse_sentence(self, input_sentence):\n",
    "        return input_sentence.reverse()\n",
    "    \n",
    "    def pad_sentence(self, input_sentence):\n",
    "        if len(input_sentence) >= MAX_FILTER_SIZE:\n",
    "            return input_sentence\n",
    "        else:\n",
    "            for i in range(MAX_FILTER_SIZE - len(input_sentence)):\n",
    "                input_sentence.append(np.zeros(WORD_SIZE))\n",
    "            return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = Generator([1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''class LSTMGAN:\n",
    "    def __init__(self):\n",
    "        self.lstm = LSTM()\n",
    "        self.d = Discriminator()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.Variable(tf.truncated_normal((1,128)))\n",
    "state = tf.Variable(tf.truncated_normal((1,128)))\n",
    "test_inputs = tf.placeholder(tf.float32, shape=[1, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_j = list()\n",
    "for _ in range(10):\n",
    "    h_j.append(tf.Variable(tf.truncated_normal(shape=(1, 10), stddev=0.3)))\n",
    "s_j = tf.Variable(tf.truncated_normal(shape=(1, 5), stddev=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_d = tf.Variable(tf.truncated_normal(shape=(10, 200)))\n",
    "s_d = tf.reshape(s_d, [1, 10, 200, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = Discriminator(s_d, [3], 64, 0.5, False,\"ddd1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST\")\n",
    "A = attention(h_j, s_j, \"143\")\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    feed_dict = dict()\n",
    "    #for i in range(3):\n",
    "        #feed_dict[L.train_data[i]] = np.reshape(test_sentence[i],(1,200))\n",
    "    outc, outa = sess.run([A.c, A.a], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.Variable([1,2])\n",
    "y = [2,2]\n",
    "z = tf.equal(x, y)\n",
    "c = tf.argmax(x,0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    a = sess.run(c)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
