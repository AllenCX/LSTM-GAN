{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "WORD_SIZE = 512\n",
    "MAX_FILTER_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocabulary_size=200, num_nodes=128, \n",
    "                 batch_size=20, num_unrollings=2, name=\"\"):\n",
    "        '''\n",
    "        the LSTM process\n",
    "        '''\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reuse = False\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "    \n",
    "    def model(self, input_sentences, vocabulary_size, num_nodes,\n",
    "             batch_size):\n",
    "        with tf.variable_scope(\"{}_LSTM_var\".format(self.name) ,reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.constant_initializer(0), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.constant_initializer(0), \n",
    "                                                       trainable=False)\n",
    "            '''self.saved_state_backward = tf.get_variable(\"saved_state_backward\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False, name=\"backward_state\")'''\n",
    "  \n",
    "        self.train_data = input_sentences\n",
    "        #Since now input_sentence is a list of word vectors, \n",
    "        #we do not need following 3 lines anymore.\n",
    "        '''for _ in range(self.num_unrollings + 1):\n",
    "            self.train_data.append(\n",
    "                tf.placeholder(tf.float32, shape=[self.batch_size, self.vocabulary_size]))'''\n",
    "        self.train_inputs = self.train_data[:-1]\n",
    "        self.train_labels = self.train_data[1:]\n",
    "        \n",
    "        self.outputs = list()\n",
    "        self.output = self.saved_output\n",
    "        self.state = self.saved_state\n",
    "        \n",
    "        self.state_list = list()\n",
    "        self.state_list.append(self.saved_state)\n",
    "        \n",
    "        for i in self.train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(self.output)\n",
    "            self.state_list.append(self.state)\n",
    "            \n",
    "        self.saved_state = self.state\n",
    "        self.saved_output = self.output\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        \n",
    "        self.reuse = True\n",
    "        \n",
    "        return self.outputs\n",
    "    def lstm_cell(self, inputs, hidden_layer, state): \n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state   \n",
    "    \n",
    "    '''def lstm_process(self, inputs, hidden_layer, state):\n",
    "        for i in train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(output)\n",
    "\n",
    "        return outputs'''\n",
    "    \n",
    "    '''with tf.control_dependencies([self.saved_output.assign(output),\n",
    "                                     self.saved_state.assign(state)]):\n",
    "            \n",
    "            logits = tf.matmul(tf.concat(0, outputs), self.w) + self.b\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, logits), \n",
    "                                                        tf.concat(0, train_labels), name=\"loss\"))\n",
    "            \n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            gradient, v = zip(*optimizer.compute_gradients(loss))\n",
    "            \n",
    "            \n",
    "            tf.add_to_collection(\"loss\", loss)\n",
    "            tf.add_to_collection(\"logits\", logits)\n",
    "            tf.add_to_collection(\"train_labels\", train_labels)\n",
    "            \n",
    "        '''\n",
    "        #It seems we do not need those lines above\n",
    "        \n",
    "    def __call__(self, input_sentences):\n",
    "        return self.model(input_sentences, self.vocabulary_size, self.num_nodes,\n",
    "                  self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST2\")\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "test_word1 = np.reshape(word_model[\"china\"], (1,200))\n",
    "test_word2 = np.reshape(word_model[\"uk\"], (1, 200))\n",
    "test_word3 = np.reshape(word_model[\"america\"], (1, 200))\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.1878904 ,  0.21309161, -0.00559057, -0.30774629,  0.14260668,\n",
       "          0.13384363, -0.03492272, -0.29711923,  0.05819021, -0.24009663,\n",
       "         -0.02576323,  0.06388636,  0.0434131 , -0.02311092, -0.10207082,\n",
       "          0.46313149, -0.24562654,  0.01214301, -0.15473817, -0.07042123,\n",
       "          0.17393345, -0.02999535,  0.32101756,  0.07122555,  0.31908828,\n",
       "         -0.21359985, -0.52953511,  0.1457652 ,  0.06813001,  0.33030874,\n",
       "         -0.3936013 , -0.18551628,  0.12033903, -0.11343805, -0.09046636,\n",
       "         -0.02878333,  0.09675267, -0.54694438,  0.11083744,  0.01935549,\n",
       "          0.19152139,  0.36283624,  0.18320811, -0.16853349, -0.08571007,\n",
       "         -0.30744252,  0.12017418,  0.33012044,  0.53576964, -0.18441848,\n",
       "         -0.04343385,  0.12184781,  0.04346642,  0.08254077, -0.01088977,\n",
       "          0.15028788,  0.27144149, -0.18214096, -0.07757476, -0.02392013,\n",
       "         -0.17091867, -0.03643473,  0.18891352,  0.53302568,  0.27142283,\n",
       "         -0.2137292 ,  0.19313948,  0.11042882, -0.17970254,  0.2370223 ,\n",
       "          0.21315947, -0.23058461, -0.03201123, -0.25588831,  0.00077013,\n",
       "          0.15616457,  0.03835293, -0.00718713, -0.04115744, -0.09138749,\n",
       "          0.16367809, -0.04318761,  0.08816456, -0.26249576, -0.22037336,\n",
       "          0.14038312, -0.09242556,  0.18005539, -0.16864325,  0.01232067,\n",
       "         -0.00465754, -0.05674632,  0.36526525,  0.3008137 , -0.23241484,\n",
       "          0.5440098 ,  0.12831624,  0.28358972,  0.08872814, -0.13357481,\n",
       "         -0.02326417, -0.27799234, -0.1732336 ,  0.09085014,  0.11579609,\n",
       "          0.13732031,  0.21824226, -0.14429933,  0.41918007,  0.06207149,\n",
       "          0.13861032,  0.18817458, -0.01525528, -0.01902742,  0.13649288,\n",
       "         -0.14024191, -0.10045656,  0.11694469,  0.12828855, -0.00370791,\n",
       "          0.05511077, -0.0056629 ,  0.04954449, -0.12656048, -0.03840916,\n",
       "          0.01306527,  0.12193085, -0.30066216]], dtype=float32),\n",
       " array([[  2.07153652e-02,  -4.57730107e-02,  -1.23401351e-01,\n",
       "          -4.31139767e-03,   4.79713753e-02,   2.71787673e-01,\n",
       "           1.11871719e-01,   1.11693824e-02,   5.23281433e-02,\n",
       "          -5.17696142e-01,  -2.45900705e-01,  -2.09680498e-01,\n",
       "           4.39580590e-01,   1.61639675e-01,   3.52413356e-01,\n",
       "           4.69865531e-01,  -1.88023224e-01,   9.76743251e-02,\n",
       "           5.60353696e-02,  -1.61186069e-01,   1.51962608e-01,\n",
       "          -1.64142288e-02,   3.39509517e-01,  -1.06518850e-01,\n",
       "           4.54766780e-01,  -9.49397832e-02,   8.80284309e-02,\n",
       "           5.12680896e-02,   1.31196409e-01,   1.31061092e-01,\n",
       "          -5.06770909e-01,  -1.31951645e-03,  -2.19931185e-01,\n",
       "          -2.04247423e-02,  -5.92670590e-02,  -2.58719921e-01,\n",
       "           1.85248494e-01,  -1.16999030e-01,   8.98826271e-02,\n",
       "           8.93994719e-02,   2.25052834e-01,   1.37341797e-01,\n",
       "          -2.57318199e-01,  -3.03708762e-01,   9.62455347e-02,\n",
       "          -3.38867307e-01,  -1.42522246e-01,  -1.50264025e-01,\n",
       "           1.48955867e-01,  -2.07142934e-01,  -1.14048645e-01,\n",
       "           9.06099007e-02,   3.55259389e-01,  -1.29163966e-01,\n",
       "           1.62379786e-01,   3.19535248e-02,   3.20761412e-01,\n",
       "          -6.77946568e-01,  -1.57159477e-01,  -1.67153706e-03,\n",
       "          -2.90442377e-01,  -6.93946984e-03,   9.89981145e-02,\n",
       "           3.74387600e-03,   2.67520666e-01,   1.40931994e-01,\n",
       "           3.80663902e-01,  -2.39819109e-01,  -2.53359586e-01,\n",
       "           6.28999546e-02,  -1.88466296e-01,  -2.50115782e-01,\n",
       "           2.58406490e-01,  -3.13497663e-01,  -5.08476160e-02,\n",
       "          -6.04031957e-04,   3.20871383e-01,   1.60000145e-01,\n",
       "          -1.31606990e-02,  -2.00219348e-01,   2.97401220e-01,\n",
       "          -1.03861183e-01,   1.33527756e-01,  -8.71175304e-02,\n",
       "           1.76010102e-01,   3.63543769e-03,   8.90048072e-02,\n",
       "           2.17713773e-01,  -4.07304674e-01,  -4.71811891e-01,\n",
       "          -8.94926563e-02,   2.41164044e-01,   4.80323136e-02,\n",
       "           1.17324963e-01,   1.03084445e-01,   2.93110996e-01,\n",
       "           4.55042958e-01,   2.73268938e-01,   4.76582378e-01,\n",
       "          -3.21633548e-01,  -2.36026887e-02,  -2.33421788e-01,\n",
       "          -5.32392621e-01,   2.10276425e-01,   4.53263856e-02,\n",
       "          -8.37739184e-02,   5.92912845e-02,   1.38152331e-01,\n",
       "           3.33454311e-02,   7.16197565e-02,   8.64690985e-04,\n",
       "           5.84217384e-02,  -4.36694533e-01,  -3.67548205e-02,\n",
       "           2.83725470e-01,  -1.15730122e-01,  -8.94084051e-02,\n",
       "           5.84760569e-02,  -1.23609737e-01,   2.73376822e-01,\n",
       "           1.62043467e-01,   2.11585574e-02,  -1.25760823e-01,\n",
       "           1.95488691e-01,  -5.30473553e-02,  -6.92377761e-02,\n",
       "           3.55356246e-01,  -5.64226657e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(L(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.20297246,  0.73646897, -0.00747686, -0.34861055,  0.26654127,\n",
       "          0.21856868, -0.07392097, -0.52268302,  0.11044142, -0.49508777,\n",
       "         -0.08273546,  0.29777533,  0.08325381, -0.05030444, -0.17717491,\n",
       "          0.64529634, -0.59762013,  0.03367092, -0.27252883, -0.44204324,\n",
       "          0.40632844, -0.0452315 ,  0.37206867,  0.33595118,  0.61236608,\n",
       "         -0.46100891, -0.69905883,  0.80436748,  0.07843294,  0.55391026,\n",
       "         -0.67953545, -0.30177727,  0.67178369, -0.14078997, -0.46405911,\n",
       "         -0.05006122,  0.33240336, -0.64243203,  0.2437918 ,  0.28983554,\n",
       "          0.43233579,  0.44700617,  0.50935811, -0.29106307, -0.18832299,\n",
       "         -0.76393771,  0.55849177,  0.46991229,  0.67351216, -0.25260225,\n",
       "         -0.31033644,  0.18070713,  0.65830761,  0.17335837, -0.0624096 ,\n",
       "          0.35236257,  0.37078151, -0.69729048, -0.10347617, -0.22809027,\n",
       "         -0.24112886, -0.07308622,  0.3032479 ,  0.75060833,  0.37919384,\n",
       "         -0.33755603,  0.66727132,  0.32290041, -0.29228994,  0.4044683 ,\n",
       "          0.87141788, -0.34827405, -0.04436417, -0.3672263 ,  0.00510728,\n",
       "          0.1763231 ,  0.04714647, -0.03504173, -0.11654708, -0.348212  ,\n",
       "          0.36601186, -0.10721587,  0.23137401, -0.43072522, -0.42995009,\n",
       "          0.34245592, -0.11096822,  0.34488282, -0.28283748,  0.01679013,\n",
       "         -0.00623674, -0.13306069,  0.68857723,  0.75576138, -0.47448868,\n",
       "          0.82010359,  0.1720131 ,  0.39939159,  0.50066614, -0.30323964,\n",
       "         -0.0330762 , -0.35898769, -0.369674  ,  0.16863486,  0.33293572,\n",
       "          0.36443725,  0.34778342, -0.22643146,  0.64022827,  0.19964026,\n",
       "          0.4767783 ,  0.2730557 , -0.03996103, -0.0271055 ,  0.37210503,\n",
       "         -0.26425159, -0.12275471,  0.35998353,  0.1898993 , -0.10277385,\n",
       "          0.05986499, -0.05347459,  0.08957218, -0.23664203, -0.12037592,\n",
       "          0.01469267,  0.19998753, -0.42720428]], dtype=float32),\n",
       " array([[ 0.0615661 , -0.09556299, -0.36407164, -0.00708805,  0.43025401,\n",
       "          0.39247209,  0.19853438,  0.36894211,  0.06373814, -0.69581103,\n",
       "         -0.43636969, -0.2927309 ,  0.66180307,  0.22864662,  0.48667186,\n",
       "          0.82166827, -0.6454215 ,  0.47361577,  0.08289787, -0.29024795,\n",
       "          0.24445489, -0.04142468,  0.39886338, -0.40350151,  0.7415992 ,\n",
       "         -0.35747445,  0.19677682,  0.12868738,  0.14160594,  0.23488876,\n",
       "         -0.93767083, -0.01976492, -0.30673465, -0.26552486, -0.39069885,\n",
       "         -0.42945409,  0.25210801, -0.33833173,  0.39093149,  0.25932997,\n",
       "          0.3739621 ,  0.58362436, -0.39592671, -0.53817636,  0.11284064,\n",
       "         -0.93669283, -0.1884166 , -0.33465284,  0.83684683, -0.36595494,\n",
       "         -0.32276955,  0.62780076,  0.49110448, -0.18187486,  0.20721802,\n",
       "          0.10705048,  0.90303338, -0.93345857, -0.50075102, -0.00216423,\n",
       "         -0.37577885, -0.01853065,  0.39715758,  0.004296  ,  0.51749188,\n",
       "          0.16373973,  0.90819043, -0.55243307, -0.55745566,  0.22237296,\n",
       "         -0.3764441 , -0.31671023,  0.3249695 , -0.62624019, -0.35460481,\n",
       "         -0.0017038 ,  0.64554846,  0.2467262 , -0.03511325, -0.37252855,\n",
       "          0.46623045, -0.36246577,  0.26353317, -0.41905791,  0.21261075,\n",
       "          0.26249567,  0.09624722,  0.61243147, -0.86992049, -0.81423736,\n",
       "         -0.30454788,  0.45201218,  0.14520967,  0.83969724,  0.38586751,\n",
       "          0.72099185,  0.62262464,  0.4676339 ,  0.69149822, -0.96743673,\n",
       "         -0.10595179, -0.32039714, -0.80340284,  0.68036473,  0.17888631,\n",
       "         -0.14932267,  0.24649793,  0.28507844,  0.07406133,  0.08604371,\n",
       "          0.02933948,  0.08260402, -0.66044796, -0.11194026,  0.31715482,\n",
       "         -0.21955019, -0.29968467,  0.11260216, -0.39077616,  0.3324039 ,\n",
       "          0.19802563,  0.0624566 , -0.30987933,  0.67805421, -0.08355948,\n",
       "         -0.11102672,  0.44778472, -0.18752968]], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(L.state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0615661 , -0.09556299, -0.36407164, -0.00708805,  0.43025401,\n",
       "         0.39247209,  0.19853438,  0.36894211,  0.06373814, -0.69581103,\n",
       "        -0.43636969, -0.2927309 ,  0.66180307,  0.22864662,  0.48667186,\n",
       "         0.82166827, -0.6454215 ,  0.47361577,  0.08289787, -0.29024795,\n",
       "         0.24445489, -0.04142468,  0.39886338, -0.40350151,  0.7415992 ,\n",
       "        -0.35747445,  0.19677682,  0.12868738,  0.14160594,  0.23488876,\n",
       "        -0.93767083, -0.01976492, -0.30673465, -0.26552486, -0.39069885,\n",
       "        -0.42945409,  0.25210801, -0.33833173,  0.39093149,  0.25932997,\n",
       "         0.3739621 ,  0.58362436, -0.39592671, -0.53817636,  0.11284064,\n",
       "        -0.93669283, -0.1884166 , -0.33465284,  0.83684683, -0.36595494,\n",
       "        -0.32276955,  0.62780076,  0.49110448, -0.18187486,  0.20721802,\n",
       "         0.10705048,  0.90303338, -0.93345857, -0.50075102, -0.00216423,\n",
       "        -0.37577885, -0.01853065,  0.39715758,  0.004296  ,  0.51749188,\n",
       "         0.16373973,  0.90819043, -0.55243307, -0.55745566,  0.22237296,\n",
       "        -0.3764441 , -0.31671023,  0.3249695 , -0.62624019, -0.35460481,\n",
       "        -0.0017038 ,  0.64554846,  0.2467262 , -0.03511325, -0.37252855,\n",
       "         0.46623045, -0.36246577,  0.26353317, -0.41905791,  0.21261075,\n",
       "         0.26249567,  0.09624722,  0.61243147, -0.86992049, -0.81423736,\n",
       "        -0.30454788,  0.45201218,  0.14520967,  0.83969724,  0.38586751,\n",
       "         0.72099185,  0.62262464,  0.4676339 ,  0.69149822, -0.96743673,\n",
       "        -0.10595179, -0.32039714, -0.80340284,  0.68036473,  0.17888631,\n",
       "        -0.14932267,  0.24649793,  0.28507844,  0.07406133,  0.08604371,\n",
       "         0.02933948,  0.08260402, -0.66044796, -0.11194026,  0.31715482,\n",
       "        -0.21955019, -0.29968467,  0.11260216, -0.39077616,  0.3324039 ,\n",
       "         0.19802563,  0.0624566 , -0.30987933,  0.67805421, -0.08355948,\n",
       "        -0.11102672,  0.44778472, -0.18752968]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(L.saved_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gen_LSTM():\n",
    "    def __init__(self, bi_hidden, max_len=32, name=\"\"):\n",
    "        self.bi_hidden = bi_hidden\n",
    "        self.name = name\n",
    "        self.reuse = False\n",
    "        \n",
    "        with tf.variable_scope(\"{}_gen_LSTM\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "                self.ic = tf.get_variable(\"ic\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.fc = tf.get_variable(\"fc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.oc = tf.get_variable(\"oc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.cc = tf.get_variable(\"cc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                \n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                self.w = tf.get_variable(\"w\", [num_nodes, WORD_SIZE], tf.float32,\n",
    "                                        tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.b = tf.get_variable(\"b\", [WORD_SIZE], tf.float32,\n",
    "                                        tf.constant_initializer(0.1))\n",
    "                \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False)\n",
    "        \n",
    "            self.c = attention(self.bi_hidden, self.saved_state)\n",
    "        state = self.saved_state\n",
    "        output = self.saved_output\n",
    "        outputs = list()\n",
    "        self.predicted_sentence = [START_WORD_VEC]\n",
    "        for i in range(max_len):\n",
    "            input_word = self.predicted_sentence[i]\n",
    "            output, state = gen_lstm_cell(input_word, output, state, self.c)\n",
    "            logits = tf.matmul(output, self.w) + self.b\n",
    "            self.predicted_sentence.append(tf.argmax(tf.nn.softmax(logits), 1))\n",
    "            if tf.equal(\n",
    "                self.predicted_sentence[-1], END_WORD_VEC).sum() == WORD_SIZE:\n",
    "                break\n",
    "            if i == max_len - 1:\n",
    "                self.predicted_sentence.append(END_WORD_VEC)\n",
    "        self.saved_state = saved_state\n",
    "        self.saved_output = saved_output\n",
    "        self.reuse = True\n",
    "        \n",
    "    def gen_lstm_cell(self, inputs, hidden_layer, state, c): \n",
    "        #single step LSTM with context vector\n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im)\\\n",
    "                                    + tf.matmul(self.ic, c) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm)\\\n",
    "                                    + tf.matmul(self.fc, c) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm)\\\n",
    "                                  + tf.matmul(self.cc, c) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om)\\\n",
    "                                     + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "            \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state\n",
    "        \n",
    "    def attention(self, h, s):\n",
    "        with tf.variable_scope(\"attention_module\", reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            for h_j in h:\n",
    "                attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                           shape=[len(h)], name=\"attention_value\")\n",
    "            c = 0\n",
    "            for j in range(int(a.get_shape()[0])):\n",
    "                c += a[j] * h[j]\n",
    "                \n",
    "            return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class attention:\n",
    "    def __init__(self, h, s, name=\"\"):\n",
    "        '''\n",
    "        Caculate the attention weight.\n",
    "        TODO:\n",
    "            I think it seems appropriate to transform the class \n",
    "            into a function...\n",
    "        arguments:\n",
    "            h: all the encoder's hidden states, \n",
    "            each of them has a size of [2*Hidden_size, Batch_size]\n",
    "            s: [2*Hidden_size, Batch_size], decoder's hidden state\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.attn_logits = list()\n",
    "        self.a = list()\n",
    "        self.reuse = False\n",
    "        \n",
    "        #some variables\n",
    "        with tf.variable_scope(\"{}_attention\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            #caculate the attention weight\n",
    "            for h_j in h:\n",
    "                self.attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                self.attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, self.attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            self.a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                                shape=[len(h)], name=\"attention_value\")\n",
    "            self.c = 0\n",
    "            for j in range(self.a.get_shape()[0]):\n",
    "                self.c += self.a[j] * h[j]\n",
    "        self.reuse = True\n",
    "    def get_attention():\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, s, conv_sizes, num_filters, dropout_keep_prob,\n",
    "                 reuse=False, name=\"\"):\n",
    "        '''\n",
    "        The Discriminator, uses Convolutional Neural Network to \n",
    "        classify the input sentence. See \n",
    "        http://www.people.fas.harvard.edu/~yoonkim/data/emnlp_2014.pdf\n",
    "        for detail. Those code below with some inspiration from\n",
    "        [https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py]\n",
    "        \n",
    "        Arguments:\n",
    "        s: the sentence representation caculated by LSTM process, \n",
    "        which has a size of [batch_size, hidden_layer_size]\n",
    "        '''\n",
    "        self.sequence_length = int(s.get_shape()[1])\n",
    "        self.reuse = reuse\n",
    "        self.name = name\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.hidden_size = int(s.get_shape()[2])\n",
    "        #print(self.hidden_size)\n",
    "        self.pooled_outputs = list()\n",
    "        for i, conv_size in enumerate(conv_sizes):\n",
    "            with tf.variable_scope(\"D_conv_maxpooling_{}\"\n",
    "                                  .format(conv_size), reuse=self.reuse):\n",
    "                #filter_shape = [conv_size, self.hidden_size, 1, num_filters]\n",
    "                W = tf.get_variable(\"W\", [conv_size, self.hidden_size, 1, num_filters],\n",
    "                                   tf.float32,\n",
    "                                   tf.truncated_normal_initializer(stddev=0.1))\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(W.name, W.get_shape()))\n",
    "                b = tf.get_variable(\"b\", [num_filters], tf.float32,\n",
    "                                   tf.constant_initializer(0.1))\n",
    "                \n",
    "                conv = tf.nn.conv2d(s, W, [1,1,1,1], 'VALID', name=\"conv\")\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(conv.name, conv.get_shape()))\n",
    "                h = tf.nn.relu(conv + b)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                       ksize=[1, self.sequence_length-conv_size+1, 1, 1],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='VALID',\n",
    "                                       name=\"max_pooling\")\n",
    "                print(\"the size of pooling layer {}\"\n",
    "                      .format(pooled.get_shape()))\n",
    "            self.pooled_outputs.append(pooled)\n",
    "            \n",
    "        self.total_filter_num = num_filters * len(conv_sizes)\n",
    "        self.h_pool = tf.concat(3, self.pooled_outputs,\n",
    "                                name=\"{}_concat_pooling_layer\".format(self.name))\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.total_filter_num])\n",
    "        print(\"h_pool size => {}\".format(self.h_pool_flat.get_shape()))\n",
    "     \n",
    "        #after convolutional layer, we can define the fully connectedmlayer\n",
    "        \n",
    "        with tf.variable_scope(\"D_fully_connected_layer\", reuse=self.reuse):\n",
    "            w_shape = [self.h_pool_flat.get_shape()[1], 1]\n",
    "            W = tf.get_variable(\"W\", w_shape, tf.float32,\n",
    "                               tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", [1], tf.float32,\n",
    "                               tf.constant_initializer(0.1))\n",
    "            logits = tf.nn.relu(tf.matmul(self.h_pool_flat, W) + b)\n",
    "            print(\"logits size => {}\".format(logits.get_shape()))\n",
    "            \n",
    "        self.reuse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, input_sentence, max_len=32, vocabulary_size=200, \n",
    "                 num_nodes=128, batch_size=16, name=\"\"):\n",
    "        self.input_sentence = input_sentence\n",
    "        self.reverse_sentence = self.reverse_sentence(self.input_sentence)\n",
    "        self.forward_LSTM_outputs = LSTM(self.input_sentence, \n",
    "                                        name=\"forward\")\n",
    "        self.backward_LSTM_outputs = LSTM(self.reverse_sentence,\n",
    "                                         name=\"backward\")     \n",
    "        #concat the two outputs \n",
    "        \n",
    "        #using attention\n",
    "        self.gen_LSTM = gen_LSTM()\n",
    "        self.attention = attention()\n",
    "        \n",
    "    def reverse_sentence(self, input_sentence):\n",
    "        return input_sentence.reverse()\n",
    "    \n",
    "    def pad_sentence(self, input_sentence):\n",
    "        if len(input_sentence) >= MAX_FILTER_SIZE:\n",
    "            return input_sentence\n",
    "        else:\n",
    "            for i in range(MAX_FILTER_SIZE - len(input_sentence)):\n",
    "                input_sentence.append(np.zeros(WORD_SIZE))\n",
    "            return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = Generator([1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''class LSTMGAN:\n",
    "    def __init__(self):\n",
    "        self.lstm = LSTM()\n",
    "        self.d = Discriminator()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.Variable(tf.truncated_normal((1,128)))\n",
    "state = tf.Variable(tf.truncated_normal((1,128)))\n",
    "test_inputs = tf.placeholder(tf.float32, shape=[1, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_j = list()\n",
    "for _ in range(10):\n",
    "    h_j.append(tf.constant(100, dtype=tf.float32, shape=[1, 10]))\n",
    "s_j = tf.constant(200, dtype=tf.float32, shape=[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = attention(h_j, s_j, \"1224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_d = tf.Variable(tf.truncated_normal((10, 200)))\n",
    "s_d = tf.reshape(s_d, [1, 10, 200, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = Discriminator(s_d, [3], 64, 0.5, False,\"ddd1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST\")\n",
    "\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    feed_dict = dict()\n",
    "    #for i in range(3):\n",
    "        #feed_dict[L.train_data[i]] = np.reshape(test_sentence[i],(1,200))\n",
    "    outc, outa = sess.run([A.c, A.a], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = tf.constant([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = np.asarray([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.Session().run(tf.equal(t, tt)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
