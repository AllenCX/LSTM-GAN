{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocabulary_size=200, num_nodes=128, batch_size=20, num_unrollings=2, name=\"\"):\n",
    "        '''\n",
    "        the LSTM process\n",
    "        '''\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reuse = False\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_var\".format(self.name) ,reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                \n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                self.w = tf.get_variable(\"w\", [num_nodes, vocabulary_size], tf.float32,\n",
    "                                        tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.b = tf.get_variable(\"b\", [vocabulary_size], tf.float32,\n",
    "                                        tf.constant_initializer(0.1))\n",
    "            \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False)\n",
    "            '''self.saved_state_backward = tf.get_variable(\"saved_state_backward\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False, name=\"backward_state\")'''\n",
    "                      \n",
    "        \n",
    "        self.train_data = list()\n",
    "        for _ in range(self.num_unrollings + 1):\n",
    "            self.train_data.append(\n",
    "                tf.placeholder(tf.float32, shape=[self.batch_size, self.vocabulary_size]))\n",
    "        self.train_inputs = self.train_data[:self.num_unrollings]\n",
    "        self.train_labels = self.train_data[1:]\n",
    "        \n",
    "        self.outputs = list()\n",
    "        self.output = self.saved_output\n",
    "        self.state = self.saved_state\n",
    "        \n",
    "        for i in self.train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(self.output)\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "    \n",
    "    def lstm_cell(self, inputs, hidden_layer, state): \n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state   \n",
    "    \n",
    "    def lstm_process(self, inputs, hidden_layer, state):\n",
    "        for i in train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(output)\n",
    "\n",
    "        return outputs\n",
    "        '''with tf.control_dependencies([self.saved_output.assign(output),\n",
    "                                     self.saved_state.assign(state)]):\n",
    "            \n",
    "            logits = tf.matmul(tf.concat(0, outputs), self.w) + self.b\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, logits), \n",
    "                                                        tf.concat(0, train_labels), name=\"loss\"))\n",
    "            \n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            gradient, v = zip(*optimizer.compute_gradients(loss))\n",
    "            \n",
    "            \n",
    "            tf.add_to_collection(\"loss\", loss)\n",
    "            tf.add_to_collection(\"logits\", logits)\n",
    "            tf.add_to_collection(\"train_labels\", train_labels)\n",
    "            \n",
    "        '''\n",
    "        #It seems we do not need those lines above\n",
    "        \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        self.lstm_process(inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class attention:\n",
    "    def __init__(self, h, s, name=\"\"):\n",
    "        '''\n",
    "        Caculate the attention weight.\n",
    "        TODO:\n",
    "            I think it seems appropriate to transform the class \n",
    "            into a function...\n",
    "        arguments:\n",
    "            h: all the encoder's hidden states, \n",
    "            each of them has a size of [2*Hidden_size, Batch_size]\n",
    "            s: [2*Hidden_size, Batch_size], decoder's hidden state\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.attn_logits = list()\n",
    "        self.a = list()\n",
    "        \n",
    "        #some variables\n",
    "        with tf.variable_scope(\"{}_attention_model\".format(self.name)):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            #caculate the attention weight\n",
    "            for h_j in h:\n",
    "                self.attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                self.attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, self.attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            self.a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                                shape=[len(h)], name=\"attention_value\")\n",
    "        \n",
    "    def get_attention():\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, s, conv_sizes, num_filters, dropout_keep_prob,\n",
    "                name=\"\"):\n",
    "        '''\n",
    "        The Discriminator, uses Convolutional Neural Network to \n",
    "        classify the input sentence. See \n",
    "        http://www.people.fas.harvard.edu/~yoonkim/data/emnlp_2014.pdf\n",
    "        for detail. Those code below with some inspiration from\n",
    "        [https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py]\n",
    "        \n",
    "        Arguments:\n",
    "        s: the sentence representation caculated by LSTM process, \n",
    "        which has a size of [batch_size, hidden_layer_size]\n",
    "        '''\n",
    "        \n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.hidden_size = int(s.get_shape()[2])\n",
    "        print(self.hidden_size)\n",
    "        self.pooled_outputs = list()\n",
    "        for i, conv_size in enumerate(conv_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpooling-{}\"\n",
    "                                  .format(conv_size)):\n",
    "                #filter_shape = [conv_size, self.hidden_size, 1, num_filters]\n",
    "                W = tf.get_variable(\"W\", [1, conv_size, 1, num_filters],\n",
    "                                   tf.float32,\n",
    "                                   tf.truncated_normal_initializer(stddev=0.1))\n",
    "                print(W.get_shape())\n",
    "                b = tf.get_variable(\"b\", [num_filters], tf.float32,\n",
    "                                   tf.constant_initializer(0.1))\n",
    "                \n",
    "                conv = tf.nn.conv2d(s, W, [1,1,1,1], 'VALID', name=\"conv\")\n",
    "                print(conv.get_shape())\n",
    "                h = tf.nn.relu(conv + b)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                       ksize=[1, 1, self.hidden_size-conv_size+1, 1],\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding='VALID',\n",
    "                                       name=\"max_pooling\")\n",
    "                print(pooled.get_shape())\n",
    "                self.pooled_outputs.append(pooled)\n",
    "                \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = [test_word1, test_word2, test_word3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.Variable(tf.truncated_normal((1,128)))\n",
    "state = tf.Variable(tf.truncated_normal((1,128)))\n",
    "test_inputs = tf.placeholder(tf.float32, shape=[1, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_j = list()\n",
    "for _ in range(10):\n",
    "    h_j.append(tf.Variable(tf.truncated_normal((1, 256))))\n",
    "s_j = tf.Variable(tf.truncated_normal((1, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = attention(h_j, s_j, \"1224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_j = tf.reshape(s_j, [1, 1, 128, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_j.get_shape()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "(1, 3, 1, 8)\n",
      "(1, 1, 126, 8)\n",
      "(1, 1, 1, 8)\n",
      "(1, 4, 1, 8)\n",
      "(1, 1, 125, 8)\n",
      "(1, 1, 1, 8)\n",
      "(1, 5, 1, 8)\n",
      "(1, 1, 124, 8)\n",
      "(1, 1, 1, 8)\n",
      "(1, 6, 1, 8)\n",
      "(1, 1, 123, 8)\n",
      "(1, 1, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator(s_j, [3, 4, 5, 6], 8, 0.5, \"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00964425, -0.23564938, -0.56542951,  0.11543711,  0.03337646,\n",
      "         0.00332987, -0.08882207,  0.01510722,  0.13301107, -0.16482908,\n",
      "        -0.02039593,  0.03089569,  0.13372445,  0.05805426, -0.00515711,\n",
      "         0.01545899, -0.09739969,  0.10948285, -0.27909574, -0.16596355,\n",
      "         0.09395462, -0.04627905,  0.12152587,  0.2531153 ,  0.61662143,\n",
      "        -0.09192392,  0.23146346,  0.01679718, -0.0521368 ,  0.01266803,\n",
      "         0.12985617,  0.40175137, -0.13689633,  0.13738512,  0.05932658,\n",
      "         0.09846523, -0.07685275, -0.05234725, -0.38297564, -0.17925805,\n",
      "        -0.17254889, -0.02166946, -0.06030574,  0.18364599,  0.07565676,\n",
      "         0.26444671,  0.03029758,  0.5055086 , -0.25924441, -0.02806696,\n",
      "         0.18840306, -0.06652591,  0.56122327,  0.02877892, -0.15657791,\n",
      "        -0.15958373, -0.05559675,  0.05910024, -0.26470396,  0.24412498,\n",
      "        -0.08462624, -0.00373746,  0.0043321 ,  0.17686912,  0.18839963,\n",
      "         0.15735877,  0.11163829, -0.20740998, -0.23735115, -0.19499524,\n",
      "        -0.39851221,  0.02788873, -0.04182589,  0.16948713,  0.07994457,\n",
      "        -0.13134919,  0.05829266,  0.13701712,  0.03635959, -0.13586093,\n",
      "        -0.22820437,  0.41410184,  0.08315182,  0.35011685, -0.00805012,\n",
      "         0.305397  ,  0.1019689 , -0.12083495,  0.09003472, -0.23859203,\n",
      "         0.10304841, -0.24628849,  0.33477834,  0.00615914,  0.06195847,\n",
      "        -0.16896746,  0.00261652, -0.19021288,  0.0069248 ,  0.24863359,\n",
      "         0.05506451,  0.08514494, -0.07996746, -0.34088397,  0.6121645 ,\n",
      "         0.09377127, -0.08417214, -0.1422534 , -0.11758046,  0.38330913,\n",
      "         0.49073032,  0.11410256,  0.09851065,  0.16602254, -0.00109081,\n",
      "         0.15917972,  0.46107665, -0.50702864, -0.32524765, -0.34738627,\n",
      "        -0.22367793, -0.14983334,  0.2179274 , -0.18182088,  0.02846572,\n",
      "        -0.08758937, -0.05688686,  0.03901193]], dtype=float32), array([[ -2.06836853e-02,  -7.67432824e-02,  -3.78107071e-01,\n",
      "         -5.99203594e-02,   2.21820511e-02,   7.06971437e-02,\n",
      "         -1.89069733e-01,  -3.47767491e-03,   1.75668642e-01,\n",
      "         -3.00488949e-01,  -6.46783188e-02,  -3.52551818e-01,\n",
      "          1.42353550e-01,   4.58816558e-01,  -1.96717396e-01,\n",
      "         -2.20743120e-01,  -2.65987188e-01,  -6.28300458e-02,\n",
      "          5.21068498e-02,   1.06491715e-01,   3.08964819e-01,\n",
      "         -6.38027340e-02,   4.07776833e-02,   4.19520199e-01,\n",
      "          1.01504795e-01,   1.01335101e-01,  -1.41463831e-01,\n",
      "          1.37976408e-01,  -4.52023037e-02,   5.95872641e-01,\n",
      "          7.97750801e-02,   5.62172294e-01,  -5.78896523e-01,\n",
      "          5.72833680e-02,   1.54724106e-01,  -2.75172919e-01,\n",
      "          1.10036448e-01,   9.29759294e-02,  -1.68567717e-01,\n",
      "          4.26764227e-02,  -7.14725778e-02,   6.96546733e-02,\n",
      "          4.52388525e-01,   1.48718461e-01,   3.85597959e-04,\n",
      "          1.02918722e-01,   3.16701174e-01,  -3.36682469e-01,\n",
      "         -2.00407490e-01,  -1.27867103e-01,   4.32303280e-01,\n",
      "         -1.41548812e-01,   3.71046990e-01,   1.47905335e-01,\n",
      "         -2.81922817e-02,  -1.96957849e-02,  -3.20657760e-01,\n",
      "          4.17119861e-01,  -1.78742707e-01,   2.42270380e-01,\n",
      "         -2.69088030e-01,  -6.28146902e-03,   1.91141337e-01,\n",
      "         -5.53387552e-02,   2.23424390e-01,   4.99988258e-01,\n",
      "          4.96086180e-02,   1.19748861e-01,  -4.12526041e-01,\n",
      "         -5.79997480e-01,  -1.89848408e-01,  -2.08325163e-01,\n",
      "          4.86865602e-02,  -3.65699790e-02,   2.47794747e-01,\n",
      "         -2.32465148e-01,   3.39236498e-01,   1.29760072e-01,\n",
      "          3.36171776e-01,   8.21811929e-02,  -5.47311723e-01,\n",
      "          5.09453043e-02,  -2.67623365e-01,   9.29732248e-02,\n",
      "         -1.40288323e-01,   1.78855270e-01,   1.97447836e-01,\n",
      "          2.19757110e-02,   1.09239928e-01,  -4.57198396e-02,\n",
      "          2.60986805e-01,  -4.89213049e-01,  -5.39283194e-02,\n",
      "         -2.09315009e-02,   1.81542844e-01,   8.34638476e-02,\n",
      "          2.31855593e-04,  -1.96050794e-04,  -4.90169674e-01,\n",
      "          5.16114151e-03,   1.12602524e-01,  -1.08882435e-01,\n",
      "         -2.13116810e-01,  -1.87543795e-01,  -1.82953089e-01,\n",
      "          7.64107099e-03,  -2.55234659e-01,   1.82806343e-01,\n",
      "         -2.04560310e-01,   1.35937065e-01,   5.73692843e-02,\n",
      "          3.26263785e-01,   2.33356208e-01,   7.09953547e-01,\n",
      "          1.36574224e-01,   1.84023261e-01,   1.35011509e-01,\n",
      "         -6.63697124e-02,  -1.04935244e-01,  -1.60787106e-01,\n",
      "          3.84964347e-02,  -1.62005424e-03,   3.95139158e-01,\n",
      "          2.35168427e-01,   1.24998555e-01,   2.49498606e-01,\n",
      "         -1.55046061e-01,   2.29605902e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    feed_dict = dict()\n",
    "    for i in range(3):\n",
    "        feed_dict[L.train_data[i]] = np.reshape(test_sentence[i],(1,200))\n",
    "    out = sess.run(L.outputs, feed_dict=feed_dict)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "1 4\n",
      "2 3\n",
      "3 2\n",
      "4 1\n"
     ]
    }
   ],
   "source": [
    "ll = [5,4,3,2,1]\n",
    "for i, l in enumerate(ll):\n",
    "    print(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
