{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_sentence, vocabulary_size=200, num_nodes=128, \n",
    "                 batch_size=20, num_unrollings=2, name=\"\"):\n",
    "        '''\n",
    "        the LSTM process\n",
    "        '''\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reuse = False\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_var\".format(self.name) ,reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "            \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False)\n",
    "            '''self.saved_state_backward = tf.get_variable(\"saved_state_backward\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False, name=\"backward_state\")'''\n",
    "                      \n",
    "        \n",
    "        \n",
    "        self.train_data = input_sentence\n",
    "        #Since now input_sentence is a list of word vectors, \n",
    "        #we do not need following 3 lines anymore.\n",
    "        '''for _ in range(self.num_unrollings + 1):\n",
    "            self.train_data.append(\n",
    "                tf.placeholder(tf.float32, shape=[self.batch_size, self.vocabulary_size]))'''\n",
    "        self.train_inputs = self.train_data[:-]\n",
    "        self.train_labels = self.train_data[1:]\n",
    "        \n",
    "        self.outputs = list()\n",
    "        self.output = self.saved_output\n",
    "        self.state = self.saved_state\n",
    "        \n",
    "        for i in self.train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(self.output)\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        self.reuse = True\n",
    "        \n",
    "    def lstm_cell(self, inputs, hidden_layer, state): \n",
    "        \n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state   \n",
    "    \n",
    "    '''def lstm_process(self, inputs, hidden_layer, state):\n",
    "        for i in train_inputs:\n",
    "            self.output, self.state = self.lstm_cell(i, self.output, self.state)\n",
    "            self.outputs.append(output)\n",
    "\n",
    "        return outputs'''\n",
    "        '''with tf.control_dependencies([self.saved_output.assign(output),\n",
    "                                     self.saved_state.assign(state)]):\n",
    "            \n",
    "            logits = tf.matmul(tf.concat(0, outputs), self.w) + self.b\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, logits), \n",
    "                                                        tf.concat(0, train_labels), name=\"loss\"))\n",
    "            \n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            gradient, v = zip(*optimizer.compute_gradients(loss))\n",
    "            \n",
    "            \n",
    "            tf.add_to_collection(\"loss\", loss)\n",
    "            tf.add_to_collection(\"logits\", logits)\n",
    "            tf.add_to_collection(\"train_labels\", train_labels)\n",
    "            \n",
    "        '''\n",
    "        #It seems we do not need those lines above\n",
    "        \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        self.lstm_process(inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gen_LSTM():\n",
    "    def __init__(self, bi_hidden, max_len=32, name=\"\"):\n",
    "        self.bi_hidden = bi_hidden\n",
    "        self.name = name\n",
    "        self.reuse = False\n",
    "        \n",
    "        with tf.variable_scope(\"{}_gen_LSTM\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            with tf.variable_scope(\"input_gate\"):\n",
    "                self.ix = tf.get_variable(\"ix\", [vocabulary_size, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.im = tf.get_variable(\"im\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ib = tf.get_variable(\"ib\", [1, num_nodes], \n",
    "                                      tf.float32, tf.constant_initializer(0.1))\n",
    "                self.ic = tf.get_variable(\"ic\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            with tf.variable_scope(\"forget_gate\"):\n",
    "                self.fx = tf.get_variable(\"fx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fm = tf.get_variable(\"fm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.fb = tf.get_variable(\"fb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.fc = tf.get_variable(\"fc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"output_gate\"):\n",
    "                self.ox = tf.get_variable(\"ox\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.om = tf.get_variable(\"om\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.ob = tf.get_variable(\"ob\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.oc = tf.get_variable(\"oc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            \n",
    "            with tf.variable_scope(\"memory_cell\"):\n",
    "                self.cx = tf.get_variable(\"cx\", [vocabulary_size, num_nodes], tf.float32, \n",
    "                                     tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cm = tf.get_variable(\"cm\", [num_nodes, num_nodes], \n",
    "                                      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.cb = tf.get_variable(\"cb\", [1, num_nodes], \n",
    "                                     tf.float32, tf.constant_initializer(0.1))\n",
    "                self.cc = tf.get_variable(\"cc\", [num_nodes*2, num_nodes],\n",
    "                                         tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "                \n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                self.w = tf.get_variable(\"w\", [num_nodes, WORD_SIZE], tf.float32,\n",
    "                                        tf.truncated_normal_initializer(stddev=0.1))\n",
    "                self.b = tf.get_variable(\"b\", [WORD_SIZE], tf.float32,\n",
    "                                        tf.constant_initializer(0.1))\n",
    "                \n",
    "            self.saved_output = tf.get_variable(\"saved_output\", [batch_size, num_nodes], \n",
    "                                                tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                trainable=False)\n",
    "            self.saved_state = tf.get_variable(\"saved_state\", [batch_size, num_nodes], \n",
    "                                                       tf.float32, tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                                       trainable=False)\n",
    "            \n",
    "        #now start the LSTM process\n",
    "            c = attention(self.bi_hidden, self.saved_state)\n",
    "            \n",
    "        self.reuse = True\n",
    "        \n",
    "    def gen_lstm_cell(self, inputs, hidden_layer, state, c): \n",
    "        #single step LSTM with context vector\n",
    "        with tf.variable_scope(\"{}_LSTM_cell\".format(self.name), reuse=self.reuse):\n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, self.ix) + tf.matmul(hidden_layer, self.im)\\\n",
    "                                    + tf.matmul(self.ic, c) + self.ib,\n",
    "                                    name=\"input_gate\")\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, self.fx) + tf.matmul(hidden_layer, self.fm)\\\n",
    "                                    + tf.matmul(self.fc, c) + self.fb,\n",
    "                                    name=\"forget_gate\")\n",
    "            update_gate = tf.tanh(tf.matmul(inputs, self.cx) + tf.matmul(hidden_layer, self.cm)\\\n",
    "                                  + tf.matmul(self.cc, c) + self.cb,\n",
    "                            name=\"update_gate\")\n",
    "            state = tf.add(tf.mul(forget_gate, state), tf.mul(input_gate, update_gate),\n",
    "                           name=\"state\")\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, self.ox) + tf.matmul(hidden_layer, self.om) + self.ob,\n",
    "                                     name=\"output_gate\") \n",
    "            output = tf.mul(output_gate, tf.tanh(state), name=\"output\")\n",
    "        \n",
    "        #Note that, the \"output\" is equal to the hidden state\n",
    "        return output, state\n",
    "        \n",
    "    def attention(self, h, s):\n",
    "        with tf.variable_scope(\"attention_module\", reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            for h_j in h:\n",
    "                attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            self.a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                                shape=[len(h)], name=\"attention_value\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class attention:\n",
    "    def __init__(self, h, s, name=\"\"):\n",
    "        '''\n",
    "        Caculate the attention weight.\n",
    "        TODO:\n",
    "            I think it seems appropriate to transform the class \n",
    "            into a function...\n",
    "        arguments:\n",
    "            h: all the encoder's hidden states, \n",
    "            each of them has a size of [2*Hidden_size, Batch_size]\n",
    "            s: [2*Hidden_size, Batch_size], decoder's hidden state\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.attn_logits = list()\n",
    "        self.a = list()\n",
    "        self.reuse = False\n",
    "        \n",
    "        #some variables\n",
    "        with tf.variable_scope(\"{}_attention\".format(self.name),\n",
    "                              reuse=self.reuse):\n",
    "            self.Ua = tf.get_variable(\"Ua\", [int(h[0].get_shape()[1])/2, int(h[0].get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            self.Wa = tf.get_variable(\"Wa\", [int(s.get_shape()[1]), int(s.get_shape()[1])],\n",
    "                                     tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "            #the following bias does not showed in original paper\n",
    "            self.Ba = tf.get_variable(\"Ba\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            self.Va = tf.get_variable(\"Va\", [1, int(s.get_shape()[1])], \n",
    "                                      tf.float32,\n",
    "                                      tf.constant_initializer(0.1))\n",
    "            \n",
    "            #caculate the attention weight\n",
    "            for h_j in h:\n",
    "                self.attn_mlp = tf.tanh(\n",
    "                    tf.matmul(self.Wa, tf.transpose(s)) + tf.matmul(self.Ua, tf.transpose(h_j)))\n",
    "                self.attn_logits.append(tf.reshape(\n",
    "                        tf.matmul(self.Va, self.attn_mlp), shape=[1])[0])\n",
    "                \n",
    "            self.a = tf.reshape(tf.nn.softmax(self.attn_logits),\n",
    "                                shape=[len(h)], name=\"attention_value\")\n",
    "        self.reuse = True\n",
    "    def get_attention():\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, s, conv_sizes, num_filters, dropout_keep_prob,\n",
    "                 reuse=False, name=\"\"):\n",
    "        '''\n",
    "        The Discriminator, uses Convolutional Neural Network to \n",
    "        classify the input sentence. See \n",
    "        http://www.people.fas.harvard.edu/~yoonkim/data/emnlp_2014.pdf\n",
    "        for detail. Those code below with some inspiration from\n",
    "        [https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py]\n",
    "        \n",
    "        Arguments:\n",
    "        s: the sentence representation caculated by LSTM process, \n",
    "        which has a size of [batch_size, hidden_layer_size]\n",
    "        '''\n",
    "        self.sequence_length = int(s.get_shape()[1])\n",
    "        self.reuse = reuse\n",
    "        self.name = name\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.hidden_size = int(s.get_shape()[2])\n",
    "        #print(self.hidden_size)\n",
    "        self.pooled_outputs = list()\n",
    "        for i, conv_size in enumerate(conv_sizes):\n",
    "            with tf.variable_scope(\"D_conv_maxpooling_{}\"\n",
    "                                  .format(conv_size), reuse=self.reuse):\n",
    "                #filter_shape = [conv_size, self.hidden_size, 1, num_filters]\n",
    "                W = tf.get_variable(\"W\", [conv_size, self.hidden_size, 1, num_filters],\n",
    "                                   tf.float32,\n",
    "                                   tf.truncated_normal_initializer(stddev=0.1))\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(W.name, W.get_shape()))\n",
    "                b = tf.get_variable(\"b\", [num_filters], tf.float32,\n",
    "                                   tf.constant_initializer(0.1))\n",
    "                \n",
    "                conv = tf.nn.conv2d(s, W, [1,1,1,1], 'VALID', name=\"conv\")\n",
    "                print(\"the size of {} => {}\"\n",
    "                      .format(conv.name, conv.get_shape()))\n",
    "                h = tf.nn.relu(conv + b)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                       ksize=[1, self.sequence_length-conv_size+1, 1, 1],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='VALID',\n",
    "                                       name=\"max_pooling\")\n",
    "                print(\"the size of pooling layer {}\"\n",
    "                      .format(pooled.get_shape()))\n",
    "            self.pooled_outputs.append(pooled)\n",
    "            \n",
    "        self.total_filter_num = num_filters * len(conv_sizes)\n",
    "        self.h_pool = tf.concat(3, self.pooled_outputs,\n",
    "                                name=\"{}_concat_pooling_layer\".format(self.name))\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.total_filter_num])\n",
    "        print(\"h_pool size => {}\".format(self.h_pool_flat.get_shape()))\n",
    "     \n",
    "        #after convolutional layer, we can define the fully connectedmlayer\n",
    "        \n",
    "        with tf.variable_scope(\"D_fully_connected_layer\", reuse=self.reuse):\n",
    "            w_shape = [self.h_pool_flat.get_shape()[1], 1]\n",
    "            W = tf.get_variable(\"W\", w_shape, tf.float32,\n",
    "                               tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", [1], tf.float32,\n",
    "                               tf.constant_initializer(0.1))\n",
    "            logits = tf.nn.relu(tf.matmul(self.h_pool_flat, W) + b)\n",
    "            print(\"logits size => {}\".format(logits.get_shape()))\n",
    "            \n",
    "        self.reuse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, input_sentence, max_len=32, vocabulary_size=200, \n",
    "                 num_nodes=128, batch_size=16, name=\"\"):\n",
    "        self.input_sentence = input_sentence\n",
    "        self.reverse_sentence = self.reverse_sentence(self.input_sentence)\n",
    "        self.forward_LSTM_outputs = LSTM(self.input_sentence, \n",
    "                                        name=\"forward\")\n",
    "        self.backward_LSTM_outputs = LSTM(self.reverse_sentence,\n",
    "                                         name=\"backward\")     \n",
    "        #concat the two outputs \n",
    "        \n",
    "        #using attention\n",
    "        self.gen_LSTM = LSTM()\n",
    "        self.attention = attention()\n",
    "        \n",
    "    def reverse_sentence(self, input_sentence):\n",
    "        return input_sentence.reverse()\n",
    "    \n",
    "    def single_step_LSTM():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''class LSTMGAN:\n",
    "    def __init__(self):\n",
    "        self.lstm = LSTM()\n",
    "        self.d = Discriminator()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.Variable(tf.truncated_normal((1,128)))\n",
    "state = tf.Variable(tf.truncated_normal((1,128)))\n",
    "test_inputs = tf.placeholder(tf.float32, shape=[1, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_j = list()\n",
    "for _ in range(10):\n",
    "    h_j.append(tf.Variable(tf.truncated_normal((1, 256))))\n",
    "s_j = tf.Variable(tf.truncated_normal((1, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = attention(h_j, s_j, \"1224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_d = tf.Variable(tf.truncated_normal((10, 200)))\n",
    "s_d = tf.reshape(s_d, [1, 10, 200, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of D_conv_maxpooling_3/W:0 => (3, 200, 1, 64)\n",
      "the size of D_conv_maxpooling_3/conv:0 => (1, 8, 1, 64)\n",
      "the size of pooling layer (1, 1, 1, 64)\n",
      "h_pool size => (1, 64)\n",
      "logits size => (1, 1)\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator(s_d, [3], 64, 0.5, False,\"ddd1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.26242559e-02,   6.03667423e-02,  -2.56971180e-01,\n",
      "          5.60867116e-02,   4.49584499e-02,   6.33594468e-02,\n",
      "         -6.29332364e-01,   7.00063333e-02,   4.59783785e-02,\n",
      "          7.34074712e-02,  -1.48096934e-01,   3.71790081e-02,\n",
      "         -9.02559701e-03,   5.46270609e-01,   2.88277343e-02,\n",
      "          1.59395486e-02,   2.25683570e-01,   1.35768533e-01,\n",
      "         -4.23966087e-02,  -2.41617516e-01,  -7.49615878e-02,\n",
      "         -2.07237810e-01,   2.72097379e-01,   4.00093585e-01,\n",
      "          2.65516229e-02,  -5.00426348e-03,  -1.25763699e-01,\n",
      "          1.30933657e-01,   3.59464320e-03,   1.26224816e-01,\n",
      "          1.57776967e-01,   5.13974018e-02,   3.45506966e-02,\n",
      "          2.67466903e-01,   6.48778304e-02,   3.05070996e-01,\n",
      "          2.41123393e-01,   2.47940704e-01,  -4.11151379e-01,\n",
      "         -7.13195419e-03,  -1.89463809e-01,  -1.07426196e-01,\n",
      "          6.85911626e-03,  -3.61409247e-01,  -3.41369361e-02,\n",
      "          5.66916689e-02,   1.10777006e-01,  -6.11392856e-02,\n",
      "         -2.06228152e-01,   3.08736861e-01,   2.55449235e-01,\n",
      "         -1.45494476e-01,  -3.22919637e-01,   3.87429446e-01,\n",
      "          4.52189036e-02,   1.30269095e-01,   1.10957190e-01,\n",
      "          5.04378617e-01,   4.13327873e-01,   1.52193084e-01,\n",
      "          3.50776851e-01,   4.11713496e-02,  -1.92944661e-01,\n",
      "          1.34275556e-01,   5.37265003e-01,   1.51984990e-01,\n",
      "         -7.54044345e-03,  -3.33883078e-03,   1.79334834e-01,\n",
      "         -1.30634174e-01,  -4.11919691e-03,  -1.74949616e-01,\n",
      "          2.40525514e-01,  -4.69627082e-02,  -9.50787496e-03,\n",
      "          1.46264464e-01,   1.01487905e-01,   1.11051179e-01,\n",
      "          8.67493972e-02,  -6.19578138e-02,   4.46097910e-01,\n",
      "          5.26505291e-01,   2.04372764e-01,   8.18616152e-03,\n",
      "         -4.43136655e-02,  -1.81401879e-01,  -4.03315902e-01,\n",
      "          2.82688677e-01,  -3.19709927e-01,  -2.21320257e-01,\n",
      "          1.25785962e-01,  -9.27653611e-02,   9.54926386e-02,\n",
      "         -2.33246297e-01,  -9.87487286e-02,  -1.55905649e-01,\n",
      "         -1.23794980e-01,   1.57194301e-01,   2.82062978e-01,\n",
      "         -6.78940117e-02,  -5.30306548e-02,  -9.58155841e-03,\n",
      "          2.38835767e-01,   3.50379407e-01,  -1.43812880e-01,\n",
      "          7.39648426e-03,  -3.63688797e-01,  -5.38678467e-01,\n",
      "         -1.02886379e-01,  -7.39601478e-02,   3.02343890e-02,\n",
      "          1.82227433e-01,   7.77505105e-04,   1.67671204e-01,\n",
      "          2.28192675e-05,   2.05355257e-01,  -7.08210170e-02,\n",
      "          3.47014189e-01,  -3.09691668e-01,   1.68900400e-01,\n",
      "         -9.73310396e-02,   2.19468698e-01,  -1.14896238e-01,\n",
      "          4.15667087e-01,   1.56636849e-01,   6.30378872e-02,\n",
      "          4.93889265e-02,  -5.38078904e-01]], dtype=float32), array([[  7.91674554e-02,   2.80900776e-01,   2.02631533e-01,\n",
      "          9.26967785e-02,  -1.18110992e-01,   3.08896869e-01,\n",
      "         -1.54273197e-01,  -3.39928180e-01,  -1.69294283e-01,\n",
      "         -1.70491904e-01,   1.09747969e-01,   5.26497364e-01,\n",
      "          4.63458151e-02,   4.30499494e-01,  -4.62534837e-02,\n",
      "          2.61527628e-01,   1.84677124e-01,  -1.11170292e-01,\n",
      "         -1.44122064e-01,  -1.68208420e-01,  -1.08364493e-01,\n",
      "         -3.80663872e-01,   5.10402732e-02,  -2.48688515e-02,\n",
      "         -7.04010352e-02,  -1.39027536e-01,  -4.99133766e-02,\n",
      "          1.45855695e-01,   2.62740646e-02,  -5.14389016e-02,\n",
      "          5.19373827e-02,  -8.58746935e-03,   3.05293053e-01,\n",
      "          2.63496280e-01,   1.20899357e-01,  -2.75001060e-02,\n",
      "          2.87040651e-01,   2.52444118e-01,  -8.18307400e-02,\n",
      "         -3.22212934e-01,  -1.58119604e-01,   1.61503524e-01,\n",
      "         -3.11299980e-01,  -5.85314035e-01,   2.30759785e-01,\n",
      "         -2.74590284e-01,  -1.18501946e-01,  -1.26054853e-01,\n",
      "          1.12448983e-01,   5.04073143e-01,  -3.10396194e-01,\n",
      "          3.86438444e-02,   4.52290922e-02,   5.13433293e-02,\n",
      "         -8.97069834e-03,   5.90275079e-02,   3.12176913e-01,\n",
      "          3.52830470e-01,   1.93917215e-01,   1.60127625e-01,\n",
      "          8.67144112e-03,   3.57347250e-01,  -5.10883406e-02,\n",
      "         -5.24869040e-02,  -8.25944357e-03,   6.62463009e-02,\n",
      "         -6.29027113e-02,  -5.28499223e-02,   1.40964508e-01,\n",
      "         -1.77280411e-01,  -3.72480571e-01,   4.93863411e-02,\n",
      "          6.58095106e-02,  -6.62653381e-03,  -5.52608573e-04,\n",
      "         -2.47487828e-01,   7.69711973e-04,   4.02790427e-01,\n",
      "         -9.76595283e-02,  -9.92856249e-02,   5.46021223e-01,\n",
      "          8.19691420e-01,   3.44354868e-01,   4.79637906e-02,\n",
      "         -2.71148290e-02,  -3.64632189e-01,   7.43916482e-02,\n",
      "          2.29391381e-01,  -2.05863312e-01,  -3.52981478e-01,\n",
      "         -4.26256597e-01,  -1.23864576e-01,  -2.17300877e-01,\n",
      "         -2.96382368e-01,  -7.32517913e-02,  -3.07955861e-01,\n",
      "         -1.32327676e-01,  -3.50617408e-03,   8.64879936e-02,\n",
      "         -1.17354719e-02,  -1.79608818e-02,   2.63115764e-03,\n",
      "          1.53251946e-01,   6.05840445e-01,   5.94565384e-02,\n",
      "          8.53390172e-02,  -2.28113472e-01,  -1.27169266e-01,\n",
      "         -4.43678826e-01,   4.41720150e-03,   7.15887770e-02,\n",
      "          5.14582992e-01,  -2.64938712e-01,   1.51248761e-02,\n",
      "          3.07769477e-01,   4.43107963e-01,  -2.71631807e-01,\n",
      "          2.82636821e-01,  -6.60669923e-01,  -1.20847806e-01,\n",
      "          2.48531431e-01,   4.41771559e-02,  -4.12438691e-01,\n",
      "          1.74308643e-01,  -1.35290906e-01,   2.57790182e-02,\n",
      "          5.80459042e-03,  -1.22831695e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "L = LSTM(batch_size=1, num_unrollings=2, name=\"TEST\")\n",
    "\n",
    "word_model = gensim.models.Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "test_word1 = word_model[\"china\"]\n",
    "test_word2 = word_model[\"uk\"]\n",
    "test_word3 = word_model[\"america\"]\n",
    "\n",
    "test_sentence = [test_word1, test_word2, test_word3]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    feed_dict = dict()\n",
    "    for i in range(3):\n",
    "        feed_dict[L.train_data[i]] = np.reshape(test_sentence[i],(1,200))\n",
    "    out = sess.run(L.outputs, feed_dict=feed_dict)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = [5,4,3,2,1]\n",
    "ll[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
